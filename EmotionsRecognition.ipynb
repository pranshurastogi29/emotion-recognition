{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionsRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CjWvnaQUrZmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "metadata": {
        "id": "ldtHMhuLrewK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "\n",
        "***Construction and Validation***\n",
        "\n",
        "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
        "\n",
        "The samples comes from:\n",
        "\n",
        "- Audio-only files;\n",
        "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "metadata": {
        "id": "JDNbxj45rkvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "metadata": {
        "id": "N-o2JI49WBAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "796aa99e-efaa-4611-c2a4-f25abc550208"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgFwaDhMbJVm",
        "colab_type": "code",
        "outputId": "19c11299-b8b5-4fec-9892-c25da63b149a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgaSHtCIdtX2",
        "colab_type": "code",
        "outputId": "d6474d00-de9a-4384-d1ba-f2a722bb3cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f97fbf9a470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEGCAYAAABxSsNVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f97fc0f3ef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vCtNuVWlr5jL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "metadata": {
        "id": "AKvuF--gd6F-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/Ravdess'\n",
        "lst = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        file = file[6:8]\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLSggnF7kKY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzvBRTJIlIE9",
        "colab_type": "code",
        "outputId": "2139450f-efb0-40cf-fea1-be26cea18db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4948, 40), (4948,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Agw-3KN1sDhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "metadata": {
        "id": "Q-Xgb5NslTBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UshLOC1ClWL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BnCR52nlXw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWyTownblZM0",
        "colab_type": "code",
        "outputId": "130cdfb5-099e-4f4e-c244-75b88ee209f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "HEuw6TUQlr7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1v0i0V7sMw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"01\",\n",
        "    \"calm\": \"02\",\n",
        "    \"happy\": \"03\",\n",
        "    \"sad\": \"04\",\n",
        "    \"angry\": \"05\", \n",
        "    \"fearful\": \"06\", \n",
        "    \"disgust\": \"07\", \n",
        "    \"surprised\": \"08\"\n",
        "}"
      ]
    },
    {
      "metadata": {
        "id": "c4kNSYkAleIv",
        "colab_type": "code",
        "outputId": "40046694-c00d-4543-c51a-2a8058cca2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       0.70      0.72      0.71       134\n",
            "          02       0.85      0.83      0.84       251\n",
            "          03       0.83      0.72      0.77       242\n",
            "          04       0.75      0.72      0.73       271\n",
            "          05       0.84      0.85      0.84       253\n",
            "          06       0.72      0.83      0.77       239\n",
            "          07       0.70      0.71      0.70       127\n",
            "          08       0.75      0.78      0.76       116\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      1633\n",
            "   macro avg       0.77      0.77      0.77      1633\n",
            "weighted avg       0.78      0.78      0.78      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lCVgjLj-gwE2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "jfaTxzZ1w__y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "metadata": {
        "id": "wcov_DCXgs7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eo0ljqzg-KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg45qSOfg-26",
        "colab_type": "code",
        "outputId": "56f1a4e1-aa12-44b7-c281-e0309bb6775f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=3, min_samples_split=20,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
              "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "aM8KU3qxhGBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "296FW5sBdanI",
        "colab_type": "code",
        "outputId": "499a20ac-0d5d-4f0b-91b6-80b1cc4ac9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       1.00      0.54      0.70       134\n",
            "          02       0.66      0.96      0.78       251\n",
            "          03       0.86      0.71      0.78       242\n",
            "          04       0.81      0.64      0.71       271\n",
            "          05       0.89      0.88      0.88       253\n",
            "          06       0.70      0.80      0.75       239\n",
            "          07       0.73      0.61      0.66       127\n",
            "          08       0.60      0.78      0.68       116\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1633\n",
            "   macro avg       0.78      0.74      0.74      1633\n",
            "weighted avg       0.79      0.76      0.76      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9eqMHV3S8i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural network"
      ]
    },
    {
      "metadata": {
        "id": "G-QscoyMxQtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build our neural network!\n",
        "\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "metadata": {
        "id": "W4i187-Pe-w5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnvoCRX1gQCh",
        "colab_type": "code",
        "outputId": "9ec5dde1-47c6-43ec-bde4-d5cb9260bf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3315, 40, 1), (1633, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "HZOGIpuefCd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6e8069b-21fe-4cbe-df88-eb0e5482833f"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LphftMIZzUvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "metadata": {
        "id": "pIWPB4Zgfic7",
        "colab_type": "code",
        "outputId": "11185b2f-ad63-4afb-e916-7be596faa5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qQSBeBhzcLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "metadata": {
        "id": "iNI1znbsfpTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktdF-nJKfq6F",
        "colab_type": "code",
        "outputId": "edd88e8c-8989-4186-ef2d-57d6c6241c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        }
      },
      "cell_type": "code",
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3315 samples, validate on 1633 samples\n",
            "Epoch 1/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 7.0924 - acc: 0.1430 - val_loss: 2.2302 - val_acc: 0.1972\n",
            "Epoch 2/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 5.2546 - acc: 0.1560 - val_loss: 2.2030 - val_acc: 0.1721\n",
            "Epoch 3/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 4.1979 - acc: 0.1780 - val_loss: 2.7677 - val_acc: 0.2070\n",
            "Epoch 4/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 3.3175 - acc: 0.1900 - val_loss: 2.1432 - val_acc: 0.2266\n",
            "Epoch 5/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 2.7242 - acc: 0.2051 - val_loss: 1.8579 - val_acc: 0.2878\n",
            "Epoch 6/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 2.3140 - acc: 0.2362 - val_loss: 1.8295 - val_acc: 0.3019\n",
            "Epoch 7/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 2.0639 - acc: 0.2434 - val_loss: 1.8248 - val_acc: 0.2694\n",
            "Epoch 8/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.9442 - acc: 0.2627 - val_loss: 1.7866 - val_acc: 0.2266\n",
            "Epoch 9/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.9041 - acc: 0.2661 - val_loss: 1.7649 - val_acc: 0.3135\n",
            "Epoch 10/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.8269 - acc: 0.3110 - val_loss: 1.7141 - val_acc: 0.3791\n",
            "Epoch 11/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.7784 - acc: 0.3164 - val_loss: 1.6804 - val_acc: 0.4011\n",
            "Epoch 12/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.7629 - acc: 0.3261 - val_loss: 1.6833 - val_acc: 0.3717\n",
            "Epoch 13/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 1.7221 - acc: 0.3406 - val_loss: 1.6413 - val_acc: 0.4115\n",
            "Epoch 14/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.7006 - acc: 0.3526 - val_loss: 1.6353 - val_acc: 0.4072\n",
            "Epoch 15/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.6715 - acc: 0.3725 - val_loss: 1.5876 - val_acc: 0.4495\n",
            "Epoch 16/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.6473 - acc: 0.3744 - val_loss: 1.6010 - val_acc: 0.3662\n",
            "Epoch 17/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.6256 - acc: 0.3891 - val_loss: 1.5694 - val_acc: 0.4660\n",
            "Epoch 18/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.6071 - acc: 0.4051 - val_loss: 1.5406 - val_acc: 0.4544\n",
            "Epoch 19/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.5914 - acc: 0.4094 - val_loss: 1.5126 - val_acc: 0.4746\n",
            "Epoch 20/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.5710 - acc: 0.4202 - val_loss: 1.5185 - val_acc: 0.4532\n",
            "Epoch 21/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.5584 - acc: 0.4193 - val_loss: 1.4906 - val_acc: 0.4685\n",
            "Epoch 22/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.5242 - acc: 0.4344 - val_loss: 1.4660 - val_acc: 0.4813\n",
            "Epoch 23/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.5241 - acc: 0.4290 - val_loss: 1.4510 - val_acc: 0.5009\n",
            "Epoch 24/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.5084 - acc: 0.4395 - val_loss: 1.4362 - val_acc: 0.5058\n",
            "Epoch 25/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 1.4914 - acc: 0.4419 - val_loss: 1.4232 - val_acc: 0.5083\n",
            "Epoch 26/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.4719 - acc: 0.4543 - val_loss: 1.4129 - val_acc: 0.4930\n",
            "Epoch 27/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.4618 - acc: 0.4588 - val_loss: 1.4072 - val_acc: 0.5168\n",
            "Epoch 28/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.4458 - acc: 0.4627 - val_loss: 1.3825 - val_acc: 0.5236\n",
            "Epoch 29/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.4252 - acc: 0.4697 - val_loss: 1.3814 - val_acc: 0.4936\n",
            "Epoch 30/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.4254 - acc: 0.4736 - val_loss: 1.3757 - val_acc: 0.5083\n",
            "Epoch 31/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3952 - acc: 0.4917 - val_loss: 1.3392 - val_acc: 0.5340\n",
            "Epoch 32/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3766 - acc: 0.4968 - val_loss: 1.4141 - val_acc: 0.4746\n",
            "Epoch 33/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.3719 - acc: 0.5077 - val_loss: 1.3195 - val_acc: 0.5242\n",
            "Epoch 34/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.3476 - acc: 0.5137 - val_loss: 1.3317 - val_acc: 0.5224\n",
            "Epoch 35/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3588 - acc: 0.4989 - val_loss: 1.3456 - val_acc: 0.5089\n",
            "Epoch 36/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.3468 - acc: 0.5104 - val_loss: 1.3203 - val_acc: 0.5340\n",
            "Epoch 37/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3364 - acc: 0.4992 - val_loss: 1.3009 - val_acc: 0.5248\n",
            "Epoch 38/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.3264 - acc: 0.5164 - val_loss: 1.2805 - val_acc: 0.5352\n",
            "Epoch 39/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3259 - acc: 0.5161 - val_loss: 1.2785 - val_acc: 0.5468\n",
            "Epoch 40/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.3028 - acc: 0.5170 - val_loss: 1.2519 - val_acc: 0.5646\n",
            "Epoch 41/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.2925 - acc: 0.5303 - val_loss: 1.2635 - val_acc: 0.5542\n",
            "Epoch 42/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.2883 - acc: 0.5294 - val_loss: 1.2502 - val_acc: 0.5609\n",
            "Epoch 43/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.2784 - acc: 0.5360 - val_loss: 1.2456 - val_acc: 0.5511\n",
            "Epoch 44/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.2679 - acc: 0.5454 - val_loss: 1.2179 - val_acc: 0.5695\n",
            "Epoch 45/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.2582 - acc: 0.5400 - val_loss: 1.2249 - val_acc: 0.5707\n",
            "Epoch 46/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.2597 - acc: 0.5321 - val_loss: 1.2376 - val_acc: 0.5456\n",
            "Epoch 47/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.2439 - acc: 0.5421 - val_loss: 1.2184 - val_acc: 0.5677\n",
            "Epoch 48/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.2347 - acc: 0.5451 - val_loss: 1.2161 - val_acc: 0.5597\n",
            "Epoch 49/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.2319 - acc: 0.5400 - val_loss: 1.1987 - val_acc: 0.5738\n",
            "Epoch 50/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.2285 - acc: 0.5493 - val_loss: 1.2066 - val_acc: 0.5677\n",
            "Epoch 51/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 1.2206 - acc: 0.5554 - val_loss: 1.1878 - val_acc: 0.5713\n",
            "Epoch 52/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 1.2061 - acc: 0.5608 - val_loss: 1.2017 - val_acc: 0.5713\n",
            "Epoch 53/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.2075 - acc: 0.5541 - val_loss: 1.1613 - val_acc: 0.5952\n",
            "Epoch 54/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1850 - acc: 0.5747 - val_loss: 1.1609 - val_acc: 0.5897\n",
            "Epoch 55/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.1680 - acc: 0.5753 - val_loss: 1.1814 - val_acc: 0.5701\n",
            "Epoch 56/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.1774 - acc: 0.5722 - val_loss: 1.1616 - val_acc: 0.5860\n",
            "Epoch 57/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.1785 - acc: 0.5722 - val_loss: 1.1506 - val_acc: 0.5818\n",
            "Epoch 58/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.1597 - acc: 0.5813 - val_loss: 1.1807 - val_acc: 0.5713\n",
            "Epoch 59/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.1556 - acc: 0.5810 - val_loss: 1.1295 - val_acc: 0.6032\n",
            "Epoch 60/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.1577 - acc: 0.5786 - val_loss: 1.1596 - val_acc: 0.5640\n",
            "Epoch 61/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.1401 - acc: 0.5771 - val_loss: 1.1335 - val_acc: 0.5964\n",
            "Epoch 62/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 1.1352 - acc: 0.5913 - val_loss: 1.1203 - val_acc: 0.5971\n",
            "Epoch 63/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.1402 - acc: 0.5738 - val_loss: 1.1262 - val_acc: 0.5934\n",
            "Epoch 64/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1353 - acc: 0.5976 - val_loss: 1.1081 - val_acc: 0.6069\n",
            "Epoch 65/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.1278 - acc: 0.5843 - val_loss: 1.1441 - val_acc: 0.5732\n",
            "Epoch 66/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.1193 - acc: 0.5916 - val_loss: 1.0959 - val_acc: 0.6197\n",
            "Epoch 67/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.1091 - acc: 0.5946 - val_loss: 1.1234 - val_acc: 0.5860\n",
            "Epoch 68/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.1120 - acc: 0.5913 - val_loss: 1.0964 - val_acc: 0.6050\n",
            "Epoch 69/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.1015 - acc: 0.5976 - val_loss: 1.0955 - val_acc: 0.6111\n",
            "Epoch 70/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.0935 - acc: 0.6078 - val_loss: 1.0814 - val_acc: 0.6179\n",
            "Epoch 71/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.0896 - acc: 0.6030 - val_loss: 1.0640 - val_acc: 0.6203\n",
            "Epoch 72/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0820 - acc: 0.6015 - val_loss: 1.0898 - val_acc: 0.6032\n",
            "Epoch 73/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0867 - acc: 0.5997 - val_loss: 1.0655 - val_acc: 0.6148\n",
            "Epoch 74/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0606 - acc: 0.6184 - val_loss: 1.0628 - val_acc: 0.6209\n",
            "Epoch 75/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0678 - acc: 0.6145 - val_loss: 1.0708 - val_acc: 0.6167\n",
            "Epoch 76/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0747 - acc: 0.6127 - val_loss: 1.0790 - val_acc: 0.6118\n",
            "Epoch 77/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0638 - acc: 0.6136 - val_loss: 1.0647 - val_acc: 0.6105\n",
            "Epoch 78/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0525 - acc: 0.6133 - val_loss: 1.0402 - val_acc: 0.6252\n",
            "Epoch 79/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.0500 - acc: 0.6145 - val_loss: 1.0431 - val_acc: 0.6216\n",
            "Epoch 80/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0461 - acc: 0.6133 - val_loss: 1.0611 - val_acc: 0.6142\n",
            "Epoch 81/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.0290 - acc: 0.6275 - val_loss: 1.0812 - val_acc: 0.5928\n",
            "Epoch 82/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0383 - acc: 0.6244 - val_loss: 1.0333 - val_acc: 0.6356\n",
            "Epoch 83/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.0271 - acc: 0.6238 - val_loss: 1.0283 - val_acc: 0.6271\n",
            "Epoch 84/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 1.0306 - acc: 0.6214 - val_loss: 1.0360 - val_acc: 0.6344\n",
            "Epoch 85/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.0125 - acc: 0.6271 - val_loss: 1.0369 - val_acc: 0.6093\n",
            "Epoch 86/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.0281 - acc: 0.6199 - val_loss: 1.0019 - val_acc: 0.6430\n",
            "Epoch 87/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 1.0190 - acc: 0.6290 - val_loss: 1.0309 - val_acc: 0.6350\n",
            "Epoch 88/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 1.0052 - acc: 0.6326 - val_loss: 0.9930 - val_acc: 0.6448\n",
            "Epoch 89/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.9998 - acc: 0.6392 - val_loss: 1.0450 - val_acc: 0.6148\n",
            "Epoch 90/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.9857 - acc: 0.6434 - val_loss: 0.9878 - val_acc: 0.6485\n",
            "Epoch 91/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.9796 - acc: 0.6425 - val_loss: 0.9934 - val_acc: 0.6485\n",
            "Epoch 92/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.9862 - acc: 0.6501 - val_loss: 0.9991 - val_acc: 0.6448\n",
            "Epoch 93/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.9955 - acc: 0.6347 - val_loss: 0.9799 - val_acc: 0.6528\n",
            "Epoch 94/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.9958 - acc: 0.6383 - val_loss: 1.0040 - val_acc: 0.6399\n",
            "Epoch 95/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.9791 - acc: 0.6456 - val_loss: 0.9765 - val_acc: 0.6571\n",
            "Epoch 96/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.9572 - acc: 0.6501 - val_loss: 0.9981 - val_acc: 0.6387\n",
            "Epoch 97/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9624 - acc: 0.6504 - val_loss: 0.9773 - val_acc: 0.6626\n",
            "Epoch 98/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9556 - acc: 0.6471 - val_loss: 1.0070 - val_acc: 0.6222\n",
            "Epoch 99/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.9641 - acc: 0.6492 - val_loss: 0.9693 - val_acc: 0.6565\n",
            "Epoch 100/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9454 - acc: 0.6516 - val_loss: 0.9618 - val_acc: 0.6558\n",
            "Epoch 101/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.9440 - acc: 0.6552 - val_loss: 0.9656 - val_acc: 0.6595\n",
            "Epoch 102/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.9347 - acc: 0.6576 - val_loss: 0.9794 - val_acc: 0.6448\n",
            "Epoch 103/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.9449 - acc: 0.6540 - val_loss: 0.9506 - val_acc: 0.6540\n",
            "Epoch 104/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.9513 - acc: 0.6516 - val_loss: 0.9545 - val_acc: 0.6540\n",
            "Epoch 105/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.9355 - acc: 0.6643 - val_loss: 0.9592 - val_acc: 0.6558\n",
            "Epoch 106/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.9288 - acc: 0.6682 - val_loss: 0.9249 - val_acc: 0.6712\n",
            "Epoch 107/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.9212 - acc: 0.6685 - val_loss: 0.9468 - val_acc: 0.6552\n",
            "Epoch 108/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.9302 - acc: 0.6591 - val_loss: 0.9376 - val_acc: 0.6669\n",
            "Epoch 109/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.9118 - acc: 0.6796 - val_loss: 0.9408 - val_acc: 0.6669\n",
            "Epoch 110/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.9030 - acc: 0.6796 - val_loss: 0.9219 - val_acc: 0.6693\n",
            "Epoch 111/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.9212 - acc: 0.6763 - val_loss: 0.9419 - val_acc: 0.6650\n",
            "Epoch 112/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.9105 - acc: 0.6691 - val_loss: 0.9655 - val_acc: 0.6479\n",
            "Epoch 113/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8979 - acc: 0.6808 - val_loss: 0.9092 - val_acc: 0.6736\n",
            "Epoch 114/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.9003 - acc: 0.6742 - val_loss: 0.9435 - val_acc: 0.6614\n",
            "Epoch 115/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8881 - acc: 0.6754 - val_loss: 0.9362 - val_acc: 0.6577\n",
            "Epoch 116/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8859 - acc: 0.6772 - val_loss: 0.9190 - val_acc: 0.6705\n",
            "Epoch 117/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8876 - acc: 0.6721 - val_loss: 0.8896 - val_acc: 0.6883\n",
            "Epoch 118/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9003 - acc: 0.6703 - val_loss: 0.9236 - val_acc: 0.6595\n",
            "Epoch 119/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8745 - acc: 0.6808 - val_loss: 0.8912 - val_acc: 0.6944\n",
            "Epoch 120/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8718 - acc: 0.6833 - val_loss: 0.9300 - val_acc: 0.6626\n",
            "Epoch 121/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8736 - acc: 0.6863 - val_loss: 0.9234 - val_acc: 0.6601\n",
            "Epoch 122/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8815 - acc: 0.6805 - val_loss: 0.8950 - val_acc: 0.6846\n",
            "Epoch 123/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8588 - acc: 0.6920 - val_loss: 0.9433 - val_acc: 0.6516\n",
            "Epoch 124/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.8575 - acc: 0.6905 - val_loss: 0.9381 - val_acc: 0.6546\n",
            "Epoch 125/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.8612 - acc: 0.6938 - val_loss: 0.8732 - val_acc: 0.6828\n",
            "Epoch 126/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.8436 - acc: 0.6977 - val_loss: 0.8755 - val_acc: 0.6859\n",
            "Epoch 127/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8415 - acc: 0.6986 - val_loss: 0.8857 - val_acc: 0.6797\n",
            "Epoch 128/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.8541 - acc: 0.6932 - val_loss: 0.9073 - val_acc: 0.6748\n",
            "Epoch 129/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8466 - acc: 0.6836 - val_loss: 0.8607 - val_acc: 0.6914\n",
            "Epoch 130/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8443 - acc: 0.6989 - val_loss: 0.8826 - val_acc: 0.6736\n",
            "Epoch 131/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8469 - acc: 0.7026 - val_loss: 0.9031 - val_acc: 0.6589\n",
            "Epoch 132/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.8442 - acc: 0.6926 - val_loss: 0.8944 - val_acc: 0.6718\n",
            "Epoch 133/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.8307 - acc: 0.6923 - val_loss: 0.9029 - val_acc: 0.6638\n",
            "Epoch 134/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.8390 - acc: 0.6968 - val_loss: 0.8538 - val_acc: 0.6950\n",
            "Epoch 135/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.8272 - acc: 0.7065 - val_loss: 0.8714 - val_acc: 0.6883\n",
            "Epoch 136/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8203 - acc: 0.7020 - val_loss: 0.8678 - val_acc: 0.6871\n",
            "Epoch 137/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.8213 - acc: 0.7062 - val_loss: 0.8519 - val_acc: 0.6944\n",
            "Epoch 138/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8143 - acc: 0.7143 - val_loss: 0.8412 - val_acc: 0.6926\n",
            "Epoch 139/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8289 - acc: 0.7026 - val_loss: 0.8640 - val_acc: 0.6797\n",
            "Epoch 140/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8165 - acc: 0.7041 - val_loss: 0.8431 - val_acc: 0.6852\n",
            "Epoch 141/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.8121 - acc: 0.7059 - val_loss: 0.8403 - val_acc: 0.6963\n",
            "Epoch 142/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.8081 - acc: 0.7044 - val_loss: 0.8384 - val_acc: 0.6963\n",
            "Epoch 143/1000\n",
            "3315/3315 [==============================] - 3s 949us/step - loss: 0.7890 - acc: 0.7149 - val_loss: 0.8350 - val_acc: 0.7048\n",
            "Epoch 144/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.8017 - acc: 0.6998 - val_loss: 0.8467 - val_acc: 0.6834\n",
            "Epoch 145/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8095 - acc: 0.7107 - val_loss: 0.8382 - val_acc: 0.6852\n",
            "Epoch 146/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7868 - acc: 0.7125 - val_loss: 0.8628 - val_acc: 0.6969\n",
            "Epoch 147/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.7862 - acc: 0.7234 - val_loss: 0.8174 - val_acc: 0.7085\n",
            "Epoch 148/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7832 - acc: 0.7122 - val_loss: 0.8378 - val_acc: 0.6950\n",
            "Epoch 149/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.7858 - acc: 0.7164 - val_loss: 0.8187 - val_acc: 0.6987\n",
            "Epoch 150/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7758 - acc: 0.7149 - val_loss: 0.8299 - val_acc: 0.7048\n",
            "Epoch 151/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7796 - acc: 0.7201 - val_loss: 0.8110 - val_acc: 0.7183\n",
            "Epoch 152/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7706 - acc: 0.7216 - val_loss: 0.8374 - val_acc: 0.6822\n",
            "Epoch 153/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7757 - acc: 0.7186 - val_loss: 0.8432 - val_acc: 0.6748\n",
            "Epoch 154/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7802 - acc: 0.7176 - val_loss: 0.8274 - val_acc: 0.7024\n",
            "Epoch 155/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7599 - acc: 0.7288 - val_loss: 0.8234 - val_acc: 0.7061\n",
            "Epoch 156/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7567 - acc: 0.7351 - val_loss: 0.8251 - val_acc: 0.6920\n",
            "Epoch 157/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7559 - acc: 0.7348 - val_loss: 0.7928 - val_acc: 0.7122\n",
            "Epoch 158/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7472 - acc: 0.7318 - val_loss: 0.8014 - val_acc: 0.7250\n",
            "Epoch 159/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.7641 - acc: 0.7167 - val_loss: 0.7978 - val_acc: 0.6999\n",
            "Epoch 160/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7585 - acc: 0.7261 - val_loss: 0.8171 - val_acc: 0.7030\n",
            "Epoch 161/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7520 - acc: 0.7279 - val_loss: 0.8063 - val_acc: 0.7214\n",
            "Epoch 162/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7432 - acc: 0.7324 - val_loss: 0.7965 - val_acc: 0.7042\n",
            "Epoch 163/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7388 - acc: 0.7324 - val_loss: 0.8322 - val_acc: 0.6920\n",
            "Epoch 164/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7466 - acc: 0.7285 - val_loss: 0.8156 - val_acc: 0.6957\n",
            "Epoch 165/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.7336 - acc: 0.7421 - val_loss: 0.7903 - val_acc: 0.7103\n",
            "Epoch 166/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7324 - acc: 0.7306 - val_loss: 0.7838 - val_acc: 0.7122\n",
            "Epoch 167/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.7253 - acc: 0.7373 - val_loss: 0.7936 - val_acc: 0.7067\n",
            "Epoch 168/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7305 - acc: 0.7385 - val_loss: 0.7832 - val_acc: 0.7257\n",
            "Epoch 169/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.7367 - acc: 0.7354 - val_loss: 0.8131 - val_acc: 0.6975\n",
            "Epoch 170/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7149 - acc: 0.7370 - val_loss: 0.7811 - val_acc: 0.7036\n",
            "Epoch 171/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7157 - acc: 0.7487 - val_loss: 0.7664 - val_acc: 0.7232\n",
            "Epoch 172/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7207 - acc: 0.7367 - val_loss: 0.7898 - val_acc: 0.7079\n",
            "Epoch 173/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7179 - acc: 0.7463 - val_loss: 0.7920 - val_acc: 0.7079\n",
            "Epoch 174/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7108 - acc: 0.7385 - val_loss: 0.7737 - val_acc: 0.7067\n",
            "Epoch 175/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.7160 - acc: 0.7412 - val_loss: 0.7665 - val_acc: 0.7226\n",
            "Epoch 176/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.7084 - acc: 0.7409 - val_loss: 0.7575 - val_acc: 0.7330\n",
            "Epoch 177/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7078 - acc: 0.7475 - val_loss: 0.7654 - val_acc: 0.7208\n",
            "Epoch 178/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.6877 - acc: 0.7551 - val_loss: 0.7851 - val_acc: 0.7085\n",
            "Epoch 179/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6923 - acc: 0.7472 - val_loss: 0.7622 - val_acc: 0.7263\n",
            "Epoch 180/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.7008 - acc: 0.7490 - val_loss: 0.7435 - val_acc: 0.7385\n",
            "Epoch 181/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.6917 - acc: 0.7538 - val_loss: 0.7409 - val_acc: 0.7306\n",
            "Epoch 182/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.7012 - acc: 0.7445 - val_loss: 0.7989 - val_acc: 0.6981\n",
            "Epoch 183/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7064 - acc: 0.7394 - val_loss: 0.7386 - val_acc: 0.7361\n",
            "Epoch 184/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6987 - acc: 0.7496 - val_loss: 0.7533 - val_acc: 0.7287\n",
            "Epoch 185/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.6872 - acc: 0.7544 - val_loss: 0.7829 - val_acc: 0.7110\n",
            "Epoch 186/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6733 - acc: 0.7544 - val_loss: 0.7405 - val_acc: 0.7306\n",
            "Epoch 187/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.6872 - acc: 0.7544 - val_loss: 0.7494 - val_acc: 0.7299\n",
            "Epoch 188/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6801 - acc: 0.7572 - val_loss: 0.7360 - val_acc: 0.7287\n",
            "Epoch 189/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.6730 - acc: 0.7523 - val_loss: 0.7357 - val_acc: 0.7355\n",
            "Epoch 190/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6822 - acc: 0.7581 - val_loss: 0.7538 - val_acc: 0.7263\n",
            "Epoch 191/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6789 - acc: 0.7560 - val_loss: 0.7464 - val_acc: 0.7134\n",
            "Epoch 192/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6664 - acc: 0.7581 - val_loss: 0.7254 - val_acc: 0.7373\n",
            "Epoch 193/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.6625 - acc: 0.7638 - val_loss: 0.7337 - val_acc: 0.7293\n",
            "Epoch 194/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6533 - acc: 0.7626 - val_loss: 0.7508 - val_acc: 0.7250\n",
            "Epoch 195/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6612 - acc: 0.7608 - val_loss: 0.7182 - val_acc: 0.7428\n",
            "Epoch 196/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6632 - acc: 0.7626 - val_loss: 0.7676 - val_acc: 0.7103\n",
            "Epoch 197/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.6519 - acc: 0.7716 - val_loss: 0.7367 - val_acc: 0.7281\n",
            "Epoch 198/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.6786 - acc: 0.7457 - val_loss: 0.7312 - val_acc: 0.7238\n",
            "Epoch 199/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6513 - acc: 0.7650 - val_loss: 0.7681 - val_acc: 0.7091\n",
            "Epoch 200/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6691 - acc: 0.7626 - val_loss: 0.7228 - val_acc: 0.7324\n",
            "Epoch 201/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6479 - acc: 0.7689 - val_loss: 0.7155 - val_acc: 0.7428\n",
            "Epoch 202/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6407 - acc: 0.7656 - val_loss: 0.7281 - val_acc: 0.7336\n",
            "Epoch 203/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6420 - acc: 0.7707 - val_loss: 0.7163 - val_acc: 0.7422\n",
            "Epoch 204/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.6468 - acc: 0.7671 - val_loss: 0.7396 - val_acc: 0.7299\n",
            "Epoch 205/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6410 - acc: 0.7653 - val_loss: 0.7029 - val_acc: 0.7502\n",
            "Epoch 206/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6382 - acc: 0.7725 - val_loss: 0.7147 - val_acc: 0.7514\n",
            "Epoch 207/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6279 - acc: 0.7810 - val_loss: 0.7148 - val_acc: 0.7373\n",
            "Epoch 208/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6330 - acc: 0.7795 - val_loss: 0.7362 - val_acc: 0.7189\n",
            "Epoch 209/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6365 - acc: 0.7710 - val_loss: 0.7026 - val_acc: 0.7453\n",
            "Epoch 210/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6235 - acc: 0.7771 - val_loss: 0.6977 - val_acc: 0.7489\n",
            "Epoch 211/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6201 - acc: 0.7771 - val_loss: 0.6955 - val_acc: 0.7575\n",
            "Epoch 212/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.6273 - acc: 0.7768 - val_loss: 0.7051 - val_acc: 0.7508\n",
            "Epoch 213/1000\n",
            "3315/3315 [==============================] - 1s 403us/step - loss: 0.6208 - acc: 0.7765 - val_loss: 0.7158 - val_acc: 0.7306\n",
            "Epoch 214/1000\n",
            "3315/3315 [==============================] - 1s 403us/step - loss: 0.6104 - acc: 0.7816 - val_loss: 0.6691 - val_acc: 0.7630\n",
            "Epoch 215/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6261 - acc: 0.7716 - val_loss: 0.6718 - val_acc: 0.7581\n",
            "Epoch 216/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6074 - acc: 0.7762 - val_loss: 0.7026 - val_acc: 0.7410\n",
            "Epoch 217/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.6193 - acc: 0.7798 - val_loss: 0.7059 - val_acc: 0.7440\n",
            "Epoch 218/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.6078 - acc: 0.7825 - val_loss: 0.6756 - val_acc: 0.7520\n",
            "Epoch 219/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6167 - acc: 0.7777 - val_loss: 0.6981 - val_acc: 0.7348\n",
            "Epoch 220/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6005 - acc: 0.7810 - val_loss: 0.7103 - val_acc: 0.7367\n",
            "Epoch 221/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.6070 - acc: 0.7876 - val_loss: 0.6851 - val_acc: 0.7477\n",
            "Epoch 222/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.6108 - acc: 0.7777 - val_loss: 0.6708 - val_acc: 0.7606\n",
            "Epoch 223/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5894 - acc: 0.7876 - val_loss: 0.6840 - val_acc: 0.7495\n",
            "Epoch 224/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6110 - acc: 0.7804 - val_loss: 0.6666 - val_acc: 0.7593\n",
            "Epoch 225/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5883 - acc: 0.7888 - val_loss: 0.7091 - val_acc: 0.7385\n",
            "Epoch 226/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.5979 - acc: 0.7861 - val_loss: 0.6805 - val_acc: 0.7636\n",
            "Epoch 227/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5957 - acc: 0.7885 - val_loss: 0.6606 - val_acc: 0.7618\n",
            "Epoch 228/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.5813 - acc: 0.7970 - val_loss: 0.6695 - val_acc: 0.7648\n",
            "Epoch 229/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.5906 - acc: 0.7873 - val_loss: 0.6844 - val_acc: 0.7514\n",
            "Epoch 230/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.5875 - acc: 0.7946 - val_loss: 0.6766 - val_acc: 0.7569\n",
            "Epoch 231/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5943 - acc: 0.7870 - val_loss: 0.6659 - val_acc: 0.7685\n",
            "Epoch 232/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.5974 - acc: 0.7913 - val_loss: 0.6720 - val_acc: 0.7593\n",
            "Epoch 233/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5822 - acc: 0.7967 - val_loss: 0.6576 - val_acc: 0.7685\n",
            "Epoch 234/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5796 - acc: 0.7955 - val_loss: 0.7083 - val_acc: 0.7544\n",
            "Epoch 235/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5680 - acc: 0.8024 - val_loss: 0.6499 - val_acc: 0.7673\n",
            "Epoch 236/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5857 - acc: 0.7943 - val_loss: 0.6745 - val_acc: 0.7495\n",
            "Epoch 237/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5582 - acc: 0.8024 - val_loss: 0.6996 - val_acc: 0.7361\n",
            "Epoch 238/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5696 - acc: 0.8006 - val_loss: 0.6591 - val_acc: 0.7624\n",
            "Epoch 239/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.5745 - acc: 0.7967 - val_loss: 0.6432 - val_acc: 0.7679\n",
            "Epoch 240/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.5819 - acc: 0.7900 - val_loss: 0.6521 - val_acc: 0.7691\n",
            "Epoch 241/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5833 - acc: 0.7888 - val_loss: 0.6670 - val_acc: 0.7520\n",
            "Epoch 242/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5554 - acc: 0.8003 - val_loss: 0.6823 - val_acc: 0.7520\n",
            "Epoch 243/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.5650 - acc: 0.8000 - val_loss: 0.6498 - val_acc: 0.7667\n",
            "Epoch 244/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.5709 - acc: 0.8006 - val_loss: 0.6426 - val_acc: 0.7661\n",
            "Epoch 245/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5558 - acc: 0.8051 - val_loss: 0.6409 - val_acc: 0.7691\n",
            "Epoch 246/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5595 - acc: 0.8045 - val_loss: 0.6690 - val_acc: 0.7428\n",
            "Epoch 247/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5543 - acc: 0.8094 - val_loss: 0.6763 - val_acc: 0.7526\n",
            "Epoch 248/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5539 - acc: 0.8030 - val_loss: 0.6382 - val_acc: 0.7722\n",
            "Epoch 249/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5493 - acc: 0.8124 - val_loss: 0.6129 - val_acc: 0.7851\n",
            "Epoch 250/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5646 - acc: 0.7988 - val_loss: 0.6363 - val_acc: 0.7722\n",
            "Epoch 251/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.5546 - acc: 0.8036 - val_loss: 0.6284 - val_acc: 0.7863\n",
            "Epoch 252/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.5510 - acc: 0.8009 - val_loss: 0.6307 - val_acc: 0.7759\n",
            "Epoch 253/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.5368 - acc: 0.8151 - val_loss: 0.6207 - val_acc: 0.7753\n",
            "Epoch 254/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.5475 - acc: 0.8018 - val_loss: 0.6369 - val_acc: 0.7685\n",
            "Epoch 255/1000\n",
            "3315/3315 [==============================] - 1s 403us/step - loss: 0.5361 - acc: 0.8148 - val_loss: 0.6222 - val_acc: 0.7783\n",
            "Epoch 256/1000\n",
            "3315/3315 [==============================] - 1s 400us/step - loss: 0.5342 - acc: 0.8097 - val_loss: 0.6130 - val_acc: 0.7765\n",
            "Epoch 257/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.5398 - acc: 0.8027 - val_loss: 0.6302 - val_acc: 0.7802\n",
            "Epoch 258/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.5317 - acc: 0.8072 - val_loss: 0.6192 - val_acc: 0.7838\n",
            "Epoch 259/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.5274 - acc: 0.8145 - val_loss: 0.6269 - val_acc: 0.7673\n",
            "Epoch 260/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.5447 - acc: 0.8081 - val_loss: 0.6417 - val_acc: 0.7685\n",
            "Epoch 261/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.5400 - acc: 0.8048 - val_loss: 0.6331 - val_acc: 0.7704\n",
            "Epoch 262/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.5437 - acc: 0.8039 - val_loss: 0.6217 - val_acc: 0.7728\n",
            "Epoch 263/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.5363 - acc: 0.8133 - val_loss: 0.6172 - val_acc: 0.7826\n",
            "Epoch 264/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5208 - acc: 0.8136 - val_loss: 0.6724 - val_acc: 0.7477\n",
            "Epoch 265/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.5173 - acc: 0.8205 - val_loss: 0.6371 - val_acc: 0.7759\n",
            "Epoch 266/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5288 - acc: 0.8087 - val_loss: 0.6084 - val_acc: 0.7875\n",
            "Epoch 267/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5307 - acc: 0.8127 - val_loss: 0.6155 - val_acc: 0.7844\n",
            "Epoch 268/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5205 - acc: 0.8193 - val_loss: 0.6140 - val_acc: 0.7789\n",
            "Epoch 269/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.5263 - acc: 0.8103 - val_loss: 0.5982 - val_acc: 0.8016\n",
            "Epoch 270/1000\n",
            "3315/3315 [==============================] - 1s 402us/step - loss: 0.5257 - acc: 0.8136 - val_loss: 0.6234 - val_acc: 0.7661\n",
            "Epoch 271/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5186 - acc: 0.8181 - val_loss: 0.6171 - val_acc: 0.7759\n",
            "Epoch 272/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5190 - acc: 0.8106 - val_loss: 0.6116 - val_acc: 0.7802\n",
            "Epoch 273/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5136 - acc: 0.8139 - val_loss: 0.5871 - val_acc: 0.7869\n",
            "Epoch 274/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5194 - acc: 0.8139 - val_loss: 0.6287 - val_acc: 0.7704\n",
            "Epoch 275/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.5186 - acc: 0.8166 - val_loss: 0.6051 - val_acc: 0.7906\n",
            "Epoch 276/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.5150 - acc: 0.8181 - val_loss: 0.5986 - val_acc: 0.7863\n",
            "Epoch 277/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.4948 - acc: 0.8244 - val_loss: 0.6257 - val_acc: 0.7704\n",
            "Epoch 278/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.5006 - acc: 0.8247 - val_loss: 0.5813 - val_acc: 0.7955\n",
            "Epoch 279/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.5047 - acc: 0.8199 - val_loss: 0.6408 - val_acc: 0.7648\n",
            "Epoch 280/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.5122 - acc: 0.8166 - val_loss: 0.5947 - val_acc: 0.7881\n",
            "Epoch 281/1000\n",
            "3315/3315 [==============================] - 1s 401us/step - loss: 0.5068 - acc: 0.8184 - val_loss: 0.6103 - val_acc: 0.7832\n",
            "Epoch 282/1000\n",
            "3315/3315 [==============================] - 1s 400us/step - loss: 0.4971 - acc: 0.8214 - val_loss: 0.6313 - val_acc: 0.7575\n",
            "Epoch 283/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.4949 - acc: 0.8275 - val_loss: 0.5841 - val_acc: 0.7906\n",
            "Epoch 284/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4949 - acc: 0.8281 - val_loss: 0.5846 - val_acc: 0.7893\n",
            "Epoch 285/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4971 - acc: 0.8244 - val_loss: 0.5624 - val_acc: 0.8034\n",
            "Epoch 286/1000\n",
            "3315/3315 [==============================] - 1s 397us/step - loss: 0.4816 - acc: 0.8302 - val_loss: 0.5761 - val_acc: 0.7998\n",
            "Epoch 287/1000\n",
            "3315/3315 [==============================] - 1s 392us/step - loss: 0.4916 - acc: 0.8320 - val_loss: 0.5786 - val_acc: 0.7924\n",
            "Epoch 288/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4769 - acc: 0.8320 - val_loss: 0.5809 - val_acc: 0.7961\n",
            "Epoch 289/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4841 - acc: 0.8329 - val_loss: 0.5831 - val_acc: 0.7869\n",
            "Epoch 290/1000\n",
            "3315/3315 [==============================] - 1s 398us/step - loss: 0.4788 - acc: 0.8281 - val_loss: 0.5640 - val_acc: 0.7906\n",
            "Epoch 291/1000\n",
            "3315/3315 [==============================] - 1s 397us/step - loss: 0.4779 - acc: 0.8253 - val_loss: 0.5767 - val_acc: 0.7985\n",
            "Epoch 292/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.4895 - acc: 0.8302 - val_loss: 0.5918 - val_acc: 0.7838\n",
            "Epoch 293/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4834 - acc: 0.8311 - val_loss: 0.5890 - val_acc: 0.7900\n",
            "Epoch 294/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4879 - acc: 0.8326 - val_loss: 0.5778 - val_acc: 0.7942\n",
            "Epoch 295/1000\n",
            "3315/3315 [==============================] - 1s 402us/step - loss: 0.4858 - acc: 0.8256 - val_loss: 0.6282 - val_acc: 0.7697\n",
            "Epoch 296/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4679 - acc: 0.8419 - val_loss: 0.5778 - val_acc: 0.7961\n",
            "Epoch 297/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.4708 - acc: 0.8377 - val_loss: 0.5804 - val_acc: 0.7918\n",
            "Epoch 298/1000\n",
            "3315/3315 [==============================] - 1s 398us/step - loss: 0.4691 - acc: 0.8338 - val_loss: 0.5646 - val_acc: 0.8034\n",
            "Epoch 299/1000\n",
            "3315/3315 [==============================] - 1s 391us/step - loss: 0.4816 - acc: 0.8350 - val_loss: 0.5958 - val_acc: 0.7881\n",
            "Epoch 300/1000\n",
            "3315/3315 [==============================] - 1s 398us/step - loss: 0.4793 - acc: 0.8308 - val_loss: 0.5686 - val_acc: 0.8022\n",
            "Epoch 301/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.4655 - acc: 0.8308 - val_loss: 0.6170 - val_acc: 0.7716\n",
            "Epoch 302/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.4734 - acc: 0.8413 - val_loss: 0.5565 - val_acc: 0.7991\n",
            "Epoch 303/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.4756 - acc: 0.8398 - val_loss: 0.5610 - val_acc: 0.7949\n",
            "Epoch 304/1000\n",
            "3315/3315 [==============================] - 1s 394us/step - loss: 0.4684 - acc: 0.8332 - val_loss: 0.5564 - val_acc: 0.8120\n",
            "Epoch 305/1000\n",
            "3315/3315 [==============================] - 1s 396us/step - loss: 0.4642 - acc: 0.8347 - val_loss: 0.5855 - val_acc: 0.7949\n",
            "Epoch 306/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.4586 - acc: 0.8452 - val_loss: 0.5689 - val_acc: 0.7942\n",
            "Epoch 307/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4568 - acc: 0.8383 - val_loss: 0.5932 - val_acc: 0.7740\n",
            "Epoch 308/1000\n",
            "3315/3315 [==============================] - 1s 400us/step - loss: 0.4473 - acc: 0.8495 - val_loss: 0.5662 - val_acc: 0.7949\n",
            "Epoch 309/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4509 - acc: 0.8422 - val_loss: 0.5625 - val_acc: 0.7967\n",
            "Epoch 310/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4492 - acc: 0.8398 - val_loss: 0.5682 - val_acc: 0.7900\n",
            "Epoch 311/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4526 - acc: 0.8380 - val_loss: 0.5478 - val_acc: 0.8016\n",
            "Epoch 312/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.4592 - acc: 0.8401 - val_loss: 0.5608 - val_acc: 0.7998\n",
            "Epoch 313/1000\n",
            "3315/3315 [==============================] - 1s 401us/step - loss: 0.4400 - acc: 0.8504 - val_loss: 0.5470 - val_acc: 0.8126\n",
            "Epoch 314/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.4440 - acc: 0.8465 - val_loss: 0.5604 - val_acc: 0.8004\n",
            "Epoch 315/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.4481 - acc: 0.8477 - val_loss: 0.5704 - val_acc: 0.7985\n",
            "Epoch 316/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4395 - acc: 0.8531 - val_loss: 0.5684 - val_acc: 0.8010\n",
            "Epoch 317/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4442 - acc: 0.8422 - val_loss: 0.5595 - val_acc: 0.7991\n",
            "Epoch 318/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.4451 - acc: 0.8483 - val_loss: 0.5404 - val_acc: 0.8157\n",
            "Epoch 319/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.4465 - acc: 0.8459 - val_loss: 0.5528 - val_acc: 0.8083\n",
            "Epoch 320/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.4522 - acc: 0.8443 - val_loss: 0.5475 - val_acc: 0.8083\n",
            "Epoch 321/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.4458 - acc: 0.8428 - val_loss: 0.5713 - val_acc: 0.7955\n",
            "Epoch 322/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.4331 - acc: 0.8543 - val_loss: 0.5397 - val_acc: 0.8102\n",
            "Epoch 323/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4437 - acc: 0.8404 - val_loss: 0.5409 - val_acc: 0.8096\n",
            "Epoch 324/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4393 - acc: 0.8498 - val_loss: 0.5285 - val_acc: 0.8249\n",
            "Epoch 325/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.4503 - acc: 0.8416 - val_loss: 0.5929 - val_acc: 0.7857\n",
            "Epoch 326/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.4306 - acc: 0.8474 - val_loss: 0.5381 - val_acc: 0.8145\n",
            "Epoch 327/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4406 - acc: 0.8492 - val_loss: 0.5219 - val_acc: 0.8169\n",
            "Epoch 328/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4297 - acc: 0.8480 - val_loss: 0.5301 - val_acc: 0.8145\n",
            "Epoch 329/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.4267 - acc: 0.8519 - val_loss: 0.5394 - val_acc: 0.8065\n",
            "Epoch 330/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4371 - acc: 0.8513 - val_loss: 0.5384 - val_acc: 0.8114\n",
            "Epoch 331/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4353 - acc: 0.8525 - val_loss: 0.5244 - val_acc: 0.8175\n",
            "Epoch 332/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4371 - acc: 0.8446 - val_loss: 0.5499 - val_acc: 0.8022\n",
            "Epoch 333/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4222 - acc: 0.8534 - val_loss: 0.5238 - val_acc: 0.8181\n",
            "Epoch 334/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4332 - acc: 0.8504 - val_loss: 0.5576 - val_acc: 0.7967\n",
            "Epoch 335/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4191 - acc: 0.8594 - val_loss: 0.5491 - val_acc: 0.8047\n",
            "Epoch 336/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4316 - acc: 0.8543 - val_loss: 0.5302 - val_acc: 0.8212\n",
            "Epoch 337/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4340 - acc: 0.8477 - val_loss: 0.5091 - val_acc: 0.8206\n",
            "Epoch 338/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.4232 - acc: 0.8540 - val_loss: 0.5244 - val_acc: 0.8163\n",
            "Epoch 339/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4203 - acc: 0.8501 - val_loss: 0.5284 - val_acc: 0.8151\n",
            "Epoch 340/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4183 - acc: 0.8579 - val_loss: 0.5259 - val_acc: 0.8120\n",
            "Epoch 341/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4214 - acc: 0.8501 - val_loss: 0.5050 - val_acc: 0.8236\n",
            "Epoch 342/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4186 - acc: 0.8537 - val_loss: 0.5273 - val_acc: 0.8200\n",
            "Epoch 343/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4219 - acc: 0.8594 - val_loss: 0.5103 - val_acc: 0.8224\n",
            "Epoch 344/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.4156 - acc: 0.8594 - val_loss: 0.5296 - val_acc: 0.8145\n",
            "Epoch 345/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4085 - acc: 0.8600 - val_loss: 0.4980 - val_acc: 0.8285\n",
            "Epoch 346/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4126 - acc: 0.8567 - val_loss: 0.5245 - val_acc: 0.8261\n",
            "Epoch 347/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4087 - acc: 0.8549 - val_loss: 0.5122 - val_acc: 0.8200\n",
            "Epoch 348/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3925 - acc: 0.8597 - val_loss: 0.5231 - val_acc: 0.8175\n",
            "Epoch 349/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4132 - acc: 0.8618 - val_loss: 0.5188 - val_acc: 0.8120\n",
            "Epoch 350/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.4144 - acc: 0.8543 - val_loss: 0.5147 - val_acc: 0.8187\n",
            "Epoch 351/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3994 - acc: 0.8606 - val_loss: 0.4923 - val_acc: 0.8383\n",
            "Epoch 352/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.4156 - acc: 0.8519 - val_loss: 0.5383 - val_acc: 0.8102\n",
            "Epoch 353/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3942 - acc: 0.8633 - val_loss: 0.4955 - val_acc: 0.8261\n",
            "Epoch 354/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.4050 - acc: 0.8549 - val_loss: 0.5178 - val_acc: 0.8230\n",
            "Epoch 355/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4104 - acc: 0.8579 - val_loss: 0.5204 - val_acc: 0.8096\n",
            "Epoch 356/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3901 - acc: 0.8630 - val_loss: 0.5064 - val_acc: 0.8230\n",
            "Epoch 357/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3891 - acc: 0.8627 - val_loss: 0.5055 - val_acc: 0.8181\n",
            "Epoch 358/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.4066 - acc: 0.8573 - val_loss: 0.5406 - val_acc: 0.8163\n",
            "Epoch 359/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4112 - acc: 0.8609 - val_loss: 0.4796 - val_acc: 0.8414\n",
            "Epoch 360/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3892 - acc: 0.8694 - val_loss: 0.5060 - val_acc: 0.8157\n",
            "Epoch 361/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.3982 - acc: 0.8570 - val_loss: 0.5120 - val_acc: 0.8120\n",
            "Epoch 362/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3904 - acc: 0.8621 - val_loss: 0.4950 - val_acc: 0.8347\n",
            "Epoch 363/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3953 - acc: 0.8588 - val_loss: 0.5007 - val_acc: 0.8249\n",
            "Epoch 364/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.4129 - acc: 0.8531 - val_loss: 0.5067 - val_acc: 0.8230\n",
            "Epoch 365/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.3951 - acc: 0.8633 - val_loss: 0.5013 - val_acc: 0.8322\n",
            "Epoch 366/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3888 - acc: 0.8612 - val_loss: 0.5299 - val_acc: 0.8169\n",
            "Epoch 367/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3837 - acc: 0.8609 - val_loss: 0.4938 - val_acc: 0.8377\n",
            "Epoch 368/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3818 - acc: 0.8673 - val_loss: 0.4943 - val_acc: 0.8224\n",
            "Epoch 369/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3878 - acc: 0.8697 - val_loss: 0.4992 - val_acc: 0.8310\n",
            "Epoch 370/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3872 - acc: 0.8652 - val_loss: 0.4816 - val_acc: 0.8255\n",
            "Epoch 371/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3801 - acc: 0.8655 - val_loss: 0.4672 - val_acc: 0.8438\n",
            "Epoch 372/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3797 - acc: 0.8661 - val_loss: 0.4828 - val_acc: 0.8402\n",
            "Epoch 373/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3808 - acc: 0.8582 - val_loss: 0.4872 - val_acc: 0.8353\n",
            "Epoch 374/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3832 - acc: 0.8679 - val_loss: 0.4922 - val_acc: 0.8242\n",
            "Epoch 375/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3656 - acc: 0.8808 - val_loss: 0.5173 - val_acc: 0.8132\n",
            "Epoch 376/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.3773 - acc: 0.8796 - val_loss: 0.5005 - val_acc: 0.8291\n",
            "Epoch 377/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3803 - acc: 0.8685 - val_loss: 0.5132 - val_acc: 0.8157\n",
            "Epoch 378/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3678 - acc: 0.8808 - val_loss: 0.4851 - val_acc: 0.8322\n",
            "Epoch 379/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3804 - acc: 0.8685 - val_loss: 0.4848 - val_acc: 0.8298\n",
            "Epoch 380/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3738 - acc: 0.8739 - val_loss: 0.4921 - val_acc: 0.8347\n",
            "Epoch 381/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3681 - acc: 0.8727 - val_loss: 0.5270 - val_acc: 0.8065\n",
            "Epoch 382/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3779 - acc: 0.8730 - val_loss: 0.4757 - val_acc: 0.8310\n",
            "Epoch 383/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3704 - acc: 0.8739 - val_loss: 0.4949 - val_acc: 0.8291\n",
            "Epoch 384/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3575 - acc: 0.8787 - val_loss: 0.5018 - val_acc: 0.8218\n",
            "Epoch 385/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3849 - acc: 0.8670 - val_loss: 0.4957 - val_acc: 0.8298\n",
            "Epoch 386/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3674 - acc: 0.8712 - val_loss: 0.4941 - val_acc: 0.8298\n",
            "Epoch 387/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.3664 - acc: 0.8751 - val_loss: 0.4796 - val_acc: 0.8396\n",
            "Epoch 388/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3584 - acc: 0.8775 - val_loss: 0.4768 - val_acc: 0.8371\n",
            "Epoch 389/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3805 - acc: 0.8712 - val_loss: 0.5013 - val_acc: 0.8212\n",
            "Epoch 390/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3675 - acc: 0.8691 - val_loss: 0.5032 - val_acc: 0.8157\n",
            "Epoch 391/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.3636 - acc: 0.8772 - val_loss: 0.4799 - val_acc: 0.8310\n",
            "Epoch 392/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.3566 - acc: 0.8778 - val_loss: 0.4635 - val_acc: 0.8402\n",
            "Epoch 393/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.3657 - acc: 0.8703 - val_loss: 0.4837 - val_acc: 0.8340\n",
            "Epoch 394/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.3521 - acc: 0.8769 - val_loss: 0.4657 - val_acc: 0.8383\n",
            "Epoch 395/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.3653 - acc: 0.8694 - val_loss: 0.4610 - val_acc: 0.8451\n",
            "Epoch 396/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3687 - acc: 0.8715 - val_loss: 0.4671 - val_acc: 0.8322\n",
            "Epoch 397/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3580 - acc: 0.8766 - val_loss: 0.4657 - val_acc: 0.8420\n",
            "Epoch 398/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3721 - acc: 0.8670 - val_loss: 0.5146 - val_acc: 0.8163\n",
            "Epoch 399/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3606 - acc: 0.8763 - val_loss: 0.4832 - val_acc: 0.8310\n",
            "Epoch 400/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3642 - acc: 0.8736 - val_loss: 0.4575 - val_acc: 0.8512\n",
            "Epoch 401/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3494 - acc: 0.8781 - val_loss: 0.4709 - val_acc: 0.8402\n",
            "Epoch 402/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.3534 - acc: 0.8830 - val_loss: 0.4918 - val_acc: 0.8328\n",
            "Epoch 403/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3461 - acc: 0.8796 - val_loss: 0.4605 - val_acc: 0.8414\n",
            "Epoch 404/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3467 - acc: 0.8790 - val_loss: 0.4654 - val_acc: 0.8396\n",
            "Epoch 405/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3603 - acc: 0.8709 - val_loss: 0.4941 - val_acc: 0.8242\n",
            "Epoch 406/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3484 - acc: 0.8799 - val_loss: 0.4639 - val_acc: 0.8438\n",
            "Epoch 407/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3409 - acc: 0.8827 - val_loss: 0.4501 - val_acc: 0.8512\n",
            "Epoch 408/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3560 - acc: 0.8793 - val_loss: 0.4842 - val_acc: 0.8347\n",
            "Epoch 409/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.3342 - acc: 0.8908 - val_loss: 0.4742 - val_acc: 0.8377\n",
            "Epoch 410/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.3361 - acc: 0.8860 - val_loss: 0.4836 - val_acc: 0.8310\n",
            "Epoch 411/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.3351 - acc: 0.8881 - val_loss: 0.4653 - val_acc: 0.8353\n",
            "Epoch 412/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3505 - acc: 0.8811 - val_loss: 0.4749 - val_acc: 0.8383\n",
            "Epoch 413/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3452 - acc: 0.8827 - val_loss: 0.4476 - val_acc: 0.8506\n",
            "Epoch 414/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3331 - acc: 0.8860 - val_loss: 0.4824 - val_acc: 0.8218\n",
            "Epoch 415/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3351 - acc: 0.8830 - val_loss: 0.4520 - val_acc: 0.8438\n",
            "Epoch 416/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3416 - acc: 0.8790 - val_loss: 0.4740 - val_acc: 0.8371\n",
            "Epoch 417/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3385 - acc: 0.8842 - val_loss: 0.4482 - val_acc: 0.8377\n",
            "Epoch 418/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3311 - acc: 0.8805 - val_loss: 0.4560 - val_acc: 0.8506\n",
            "Epoch 419/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.3383 - acc: 0.8845 - val_loss: 0.4632 - val_acc: 0.8420\n",
            "Epoch 420/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.3286 - acc: 0.8878 - val_loss: 0.4668 - val_acc: 0.8475\n",
            "Epoch 421/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.3473 - acc: 0.8808 - val_loss: 0.4616 - val_acc: 0.8463\n",
            "Epoch 422/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.3330 - acc: 0.8857 - val_loss: 0.4553 - val_acc: 0.8475\n",
            "Epoch 423/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3350 - acc: 0.8842 - val_loss: 0.4446 - val_acc: 0.8481\n",
            "Epoch 424/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3324 - acc: 0.8836 - val_loss: 0.4515 - val_acc: 0.8500\n",
            "Epoch 425/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3338 - acc: 0.8899 - val_loss: 0.4694 - val_acc: 0.8377\n",
            "Epoch 426/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.3297 - acc: 0.8863 - val_loss: 0.4569 - val_acc: 0.8420\n",
            "Epoch 427/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3401 - acc: 0.8833 - val_loss: 0.4306 - val_acc: 0.8561\n",
            "Epoch 428/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.3391 - acc: 0.8854 - val_loss: 0.4385 - val_acc: 0.8481\n",
            "Epoch 429/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3367 - acc: 0.8845 - val_loss: 0.4486 - val_acc: 0.8524\n",
            "Epoch 430/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3325 - acc: 0.8860 - val_loss: 0.4623 - val_acc: 0.8383\n",
            "Epoch 431/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3243 - acc: 0.8932 - val_loss: 0.4411 - val_acc: 0.8579\n",
            "Epoch 432/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3380 - acc: 0.8814 - val_loss: 0.4450 - val_acc: 0.8518\n",
            "Epoch 433/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3140 - acc: 0.8878 - val_loss: 0.4421 - val_acc: 0.8506\n",
            "Epoch 434/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3203 - acc: 0.8899 - val_loss: 0.4666 - val_acc: 0.8414\n",
            "Epoch 435/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3273 - acc: 0.8857 - val_loss: 0.4641 - val_acc: 0.8408\n",
            "Epoch 436/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3347 - acc: 0.8836 - val_loss: 0.4404 - val_acc: 0.8426\n",
            "Epoch 437/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3134 - acc: 0.8938 - val_loss: 0.4511 - val_acc: 0.8445\n",
            "Epoch 438/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3358 - acc: 0.8845 - val_loss: 0.4258 - val_acc: 0.8567\n",
            "Epoch 439/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3224 - acc: 0.8881 - val_loss: 0.4204 - val_acc: 0.8592\n",
            "Epoch 440/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3287 - acc: 0.8884 - val_loss: 0.4346 - val_acc: 0.8573\n",
            "Epoch 441/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3246 - acc: 0.8908 - val_loss: 0.4423 - val_acc: 0.8506\n",
            "Epoch 442/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3114 - acc: 0.8941 - val_loss: 0.4383 - val_acc: 0.8451\n",
            "Epoch 443/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.3148 - acc: 0.8959 - val_loss: 0.4471 - val_acc: 0.8549\n",
            "Epoch 444/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3123 - acc: 0.8938 - val_loss: 0.4301 - val_acc: 0.8518\n",
            "Epoch 445/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3221 - acc: 0.8887 - val_loss: 0.4506 - val_acc: 0.8457\n",
            "Epoch 446/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3142 - acc: 0.8932 - val_loss: 0.4815 - val_acc: 0.8249\n",
            "Epoch 447/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3157 - acc: 0.8875 - val_loss: 0.4200 - val_acc: 0.8567\n",
            "Epoch 448/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2992 - acc: 0.8989 - val_loss: 0.4234 - val_acc: 0.8610\n",
            "Epoch 449/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3098 - acc: 0.9011 - val_loss: 0.4280 - val_acc: 0.8549\n",
            "Epoch 450/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2976 - acc: 0.8998 - val_loss: 0.4375 - val_acc: 0.8610\n",
            "Epoch 451/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3225 - acc: 0.8887 - val_loss: 0.4323 - val_acc: 0.8585\n",
            "Epoch 452/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3154 - acc: 0.8914 - val_loss: 0.4154 - val_acc: 0.8598\n",
            "Epoch 453/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2920 - acc: 0.9056 - val_loss: 0.4237 - val_acc: 0.8530\n",
            "Epoch 454/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3055 - acc: 0.8932 - val_loss: 0.4309 - val_acc: 0.8494\n",
            "Epoch 455/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3112 - acc: 0.8911 - val_loss: 0.4620 - val_acc: 0.8383\n",
            "Epoch 456/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3108 - acc: 0.8926 - val_loss: 0.4374 - val_acc: 0.8494\n",
            "Epoch 457/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3029 - acc: 0.8959 - val_loss: 0.4244 - val_acc: 0.8628\n",
            "Epoch 458/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3152 - acc: 0.8863 - val_loss: 0.4117 - val_acc: 0.8665\n",
            "Epoch 459/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.3082 - acc: 0.8938 - val_loss: 0.4224 - val_acc: 0.8592\n",
            "Epoch 460/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3093 - acc: 0.8974 - val_loss: 0.4321 - val_acc: 0.8506\n",
            "Epoch 461/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3004 - acc: 0.8938 - val_loss: 0.4175 - val_acc: 0.8659\n",
            "Epoch 462/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.3054 - acc: 0.8980 - val_loss: 0.4317 - val_acc: 0.8573\n",
            "Epoch 463/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3065 - acc: 0.8899 - val_loss: 0.4266 - val_acc: 0.8598\n",
            "Epoch 464/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2952 - acc: 0.9011 - val_loss: 0.4196 - val_acc: 0.8610\n",
            "Epoch 465/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3000 - acc: 0.8941 - val_loss: 0.4292 - val_acc: 0.8634\n",
            "Epoch 466/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3014 - acc: 0.8992 - val_loss: 0.4104 - val_acc: 0.8647\n",
            "Epoch 467/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2993 - acc: 0.8992 - val_loss: 0.4204 - val_acc: 0.8628\n",
            "Epoch 468/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3119 - acc: 0.8911 - val_loss: 0.4101 - val_acc: 0.8598\n",
            "Epoch 469/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2916 - acc: 0.8995 - val_loss: 0.4229 - val_acc: 0.8677\n",
            "Epoch 470/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3061 - acc: 0.8938 - val_loss: 0.4276 - val_acc: 0.8665\n",
            "Epoch 471/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2959 - acc: 0.9020 - val_loss: 0.3981 - val_acc: 0.8634\n",
            "Epoch 472/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2853 - acc: 0.9044 - val_loss: 0.4116 - val_acc: 0.8585\n",
            "Epoch 473/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3030 - acc: 0.8956 - val_loss: 0.4286 - val_acc: 0.8628\n",
            "Epoch 474/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2910 - acc: 0.8989 - val_loss: 0.4112 - val_acc: 0.8653\n",
            "Epoch 475/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2955 - acc: 0.8992 - val_loss: 0.4361 - val_acc: 0.8457\n",
            "Epoch 476/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2821 - acc: 0.9056 - val_loss: 0.4369 - val_acc: 0.8512\n",
            "Epoch 477/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2910 - acc: 0.9029 - val_loss: 0.4145 - val_acc: 0.8671\n",
            "Epoch 478/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2949 - acc: 0.9011 - val_loss: 0.3918 - val_acc: 0.8720\n",
            "Epoch 479/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2912 - acc: 0.9059 - val_loss: 0.4144 - val_acc: 0.8634\n",
            "Epoch 480/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2937 - acc: 0.8998 - val_loss: 0.4189 - val_acc: 0.8524\n",
            "Epoch 481/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3031 - acc: 0.8974 - val_loss: 0.4365 - val_acc: 0.8481\n",
            "Epoch 482/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2820 - acc: 0.9026 - val_loss: 0.4147 - val_acc: 0.8653\n",
            "Epoch 483/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2924 - acc: 0.9029 - val_loss: 0.4067 - val_acc: 0.8683\n",
            "Epoch 484/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2859 - acc: 0.9032 - val_loss: 0.4188 - val_acc: 0.8604\n",
            "Epoch 485/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2904 - acc: 0.8962 - val_loss: 0.4170 - val_acc: 0.8604\n",
            "Epoch 486/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2791 - acc: 0.9050 - val_loss: 0.4183 - val_acc: 0.8714\n",
            "Epoch 487/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2704 - acc: 0.9059 - val_loss: 0.4237 - val_acc: 0.8555\n",
            "Epoch 488/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2900 - acc: 0.9029 - val_loss: 0.4322 - val_acc: 0.8500\n",
            "Epoch 489/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2919 - acc: 0.9041 - val_loss: 0.4387 - val_acc: 0.8494\n",
            "Epoch 490/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2884 - acc: 0.8989 - val_loss: 0.4143 - val_acc: 0.8561\n",
            "Epoch 491/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2872 - acc: 0.9023 - val_loss: 0.4286 - val_acc: 0.8567\n",
            "Epoch 492/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2701 - acc: 0.9062 - val_loss: 0.4232 - val_acc: 0.8543\n",
            "Epoch 493/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2744 - acc: 0.9095 - val_loss: 0.4167 - val_acc: 0.8567\n",
            "Epoch 494/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2774 - acc: 0.9011 - val_loss: 0.3958 - val_acc: 0.8671\n",
            "Epoch 495/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2877 - acc: 0.9065 - val_loss: 0.4454 - val_acc: 0.8487\n",
            "Epoch 496/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2720 - acc: 0.9071 - val_loss: 0.4292 - val_acc: 0.8500\n",
            "Epoch 497/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2778 - acc: 0.9044 - val_loss: 0.3854 - val_acc: 0.8708\n",
            "Epoch 498/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2623 - acc: 0.9074 - val_loss: 0.3909 - val_acc: 0.8757\n",
            "Epoch 499/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2863 - acc: 0.9005 - val_loss: 0.4142 - val_acc: 0.8616\n",
            "Epoch 500/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2749 - acc: 0.9029 - val_loss: 0.3964 - val_acc: 0.8677\n",
            "Epoch 501/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2856 - acc: 0.8995 - val_loss: 0.4111 - val_acc: 0.8641\n",
            "Epoch 502/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2760 - acc: 0.9077 - val_loss: 0.4027 - val_acc: 0.8573\n",
            "Epoch 503/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2617 - acc: 0.9137 - val_loss: 0.4150 - val_acc: 0.8671\n",
            "Epoch 504/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2739 - acc: 0.9095 - val_loss: 0.4157 - val_acc: 0.8585\n",
            "Epoch 505/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2708 - acc: 0.9086 - val_loss: 0.4069 - val_acc: 0.8628\n",
            "Epoch 506/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2668 - acc: 0.9089 - val_loss: 0.3972 - val_acc: 0.8659\n",
            "Epoch 507/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2749 - acc: 0.9041 - val_loss: 0.4151 - val_acc: 0.8641\n",
            "Epoch 508/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2680 - acc: 0.9092 - val_loss: 0.4300 - val_acc: 0.8500\n",
            "Epoch 509/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2797 - acc: 0.9002 - val_loss: 0.3947 - val_acc: 0.8739\n",
            "Epoch 510/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2687 - acc: 0.9101 - val_loss: 0.4023 - val_acc: 0.8579\n",
            "Epoch 511/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2564 - acc: 0.9125 - val_loss: 0.4177 - val_acc: 0.8610\n",
            "Epoch 512/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2737 - acc: 0.9080 - val_loss: 0.4135 - val_acc: 0.8622\n",
            "Epoch 513/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2590 - acc: 0.9131 - val_loss: 0.4075 - val_acc: 0.8543\n",
            "Epoch 514/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2745 - acc: 0.8995 - val_loss: 0.3823 - val_acc: 0.8720\n",
            "Epoch 515/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2628 - acc: 0.9083 - val_loss: 0.4249 - val_acc: 0.8549\n",
            "Epoch 516/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2813 - acc: 0.9053 - val_loss: 0.3901 - val_acc: 0.8745\n",
            "Epoch 517/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2559 - acc: 0.9149 - val_loss: 0.3958 - val_acc: 0.8739\n",
            "Epoch 518/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2711 - acc: 0.9044 - val_loss: 0.4072 - val_acc: 0.8696\n",
            "Epoch 519/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2628 - acc: 0.9068 - val_loss: 0.3950 - val_acc: 0.8788\n",
            "Epoch 520/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2630 - acc: 0.8974 - val_loss: 0.4234 - val_acc: 0.8579\n",
            "Epoch 521/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2589 - acc: 0.9077 - val_loss: 0.4140 - val_acc: 0.8628\n",
            "Epoch 522/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2437 - acc: 0.9210 - val_loss: 0.3882 - val_acc: 0.8781\n",
            "Epoch 523/1000\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2698 - acc: 0.9074 - val_loss: 0.3972 - val_acc: 0.8763\n",
            "Epoch 524/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2681 - acc: 0.9068 - val_loss: 0.4076 - val_acc: 0.8739\n",
            "Epoch 525/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2703 - acc: 0.9092 - val_loss: 0.4183 - val_acc: 0.8524\n",
            "Epoch 526/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2581 - acc: 0.9149 - val_loss: 0.3829 - val_acc: 0.8775\n",
            "Epoch 527/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2578 - acc: 0.9113 - val_loss: 0.3999 - val_acc: 0.8732\n",
            "Epoch 528/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2567 - acc: 0.9125 - val_loss: 0.3963 - val_acc: 0.8726\n",
            "Epoch 529/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2624 - acc: 0.9071 - val_loss: 0.3820 - val_acc: 0.8745\n",
            "Epoch 530/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2578 - acc: 0.9176 - val_loss: 0.4107 - val_acc: 0.8598\n",
            "Epoch 531/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2528 - acc: 0.9155 - val_loss: 0.3849 - val_acc: 0.8671\n",
            "Epoch 532/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2503 - acc: 0.9176 - val_loss: 0.3788 - val_acc: 0.8769\n",
            "Epoch 533/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2446 - acc: 0.9176 - val_loss: 0.3851 - val_acc: 0.8775\n",
            "Epoch 534/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2530 - acc: 0.9128 - val_loss: 0.3757 - val_acc: 0.8788\n",
            "Epoch 535/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2590 - acc: 0.9152 - val_loss: 0.3877 - val_acc: 0.8714\n",
            "Epoch 536/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2434 - acc: 0.9173 - val_loss: 0.3925 - val_acc: 0.8757\n",
            "Epoch 537/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2642 - acc: 0.9065 - val_loss: 0.3906 - val_acc: 0.8855\n",
            "Epoch 538/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2484 - acc: 0.9164 - val_loss: 0.3883 - val_acc: 0.8726\n",
            "Epoch 539/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2464 - acc: 0.9101 - val_loss: 0.3739 - val_acc: 0.8739\n",
            "Epoch 540/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2343 - acc: 0.9207 - val_loss: 0.3894 - val_acc: 0.8726\n",
            "Epoch 541/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2413 - acc: 0.9143 - val_loss: 0.4128 - val_acc: 0.8702\n",
            "Epoch 542/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2342 - acc: 0.9228 - val_loss: 0.4248 - val_acc: 0.8585\n",
            "Epoch 543/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2576 - acc: 0.9131 - val_loss: 0.3978 - val_acc: 0.8690\n",
            "Epoch 544/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2613 - acc: 0.9107 - val_loss: 0.3732 - val_acc: 0.8843\n",
            "Epoch 545/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2560 - acc: 0.9131 - val_loss: 0.3902 - val_acc: 0.8781\n",
            "Epoch 546/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2437 - acc: 0.9134 - val_loss: 0.3713 - val_acc: 0.8794\n",
            "Epoch 547/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.2300 - acc: 0.9219 - val_loss: 0.3974 - val_acc: 0.8788\n",
            "Epoch 548/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2537 - acc: 0.9149 - val_loss: 0.3749 - val_acc: 0.8763\n",
            "Epoch 549/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2318 - acc: 0.9170 - val_loss: 0.3919 - val_acc: 0.8775\n",
            "Epoch 550/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2404 - acc: 0.9219 - val_loss: 0.3901 - val_acc: 0.8677\n",
            "Epoch 551/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2311 - acc: 0.9222 - val_loss: 0.4094 - val_acc: 0.8683\n",
            "Epoch 552/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2486 - acc: 0.9122 - val_loss: 0.4122 - val_acc: 0.8732\n",
            "Epoch 553/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2390 - acc: 0.9219 - val_loss: 0.3716 - val_acc: 0.8824\n",
            "Epoch 554/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2354 - acc: 0.9258 - val_loss: 0.3916 - val_acc: 0.8708\n",
            "Epoch 555/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2505 - acc: 0.9113 - val_loss: 0.3757 - val_acc: 0.8824\n",
            "Epoch 556/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2364 - acc: 0.9164 - val_loss: 0.3785 - val_acc: 0.8824\n",
            "Epoch 557/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2438 - acc: 0.9158 - val_loss: 0.3774 - val_acc: 0.8788\n",
            "Epoch 558/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2343 - acc: 0.9201 - val_loss: 0.3944 - val_acc: 0.8665\n",
            "Epoch 559/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2352 - acc: 0.9195 - val_loss: 0.3768 - val_acc: 0.8873\n",
            "Epoch 560/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2386 - acc: 0.9173 - val_loss: 0.3958 - val_acc: 0.8616\n",
            "Epoch 561/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2417 - acc: 0.9195 - val_loss: 0.3603 - val_acc: 0.8830\n",
            "Epoch 562/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2380 - acc: 0.9149 - val_loss: 0.3676 - val_acc: 0.8824\n",
            "Epoch 563/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2306 - acc: 0.9225 - val_loss: 0.3919 - val_acc: 0.8641\n",
            "Epoch 564/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2433 - acc: 0.9173 - val_loss: 0.3721 - val_acc: 0.8818\n",
            "Epoch 565/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2456 - acc: 0.9161 - val_loss: 0.3804 - val_acc: 0.8861\n",
            "Epoch 566/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2343 - acc: 0.9204 - val_loss: 0.3681 - val_acc: 0.8824\n",
            "Epoch 567/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2381 - acc: 0.9192 - val_loss: 0.3926 - val_acc: 0.8683\n",
            "Epoch 568/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2320 - acc: 0.9186 - val_loss: 0.3704 - val_acc: 0.8873\n",
            "Epoch 569/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2396 - acc: 0.9170 - val_loss: 0.3714 - val_acc: 0.8806\n",
            "Epoch 570/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2279 - acc: 0.9183 - val_loss: 0.3617 - val_acc: 0.8879\n",
            "Epoch 571/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2402 - acc: 0.9155 - val_loss: 0.4021 - val_acc: 0.8671\n",
            "Epoch 572/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2308 - acc: 0.9240 - val_loss: 0.3607 - val_acc: 0.8818\n",
            "Epoch 573/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2366 - acc: 0.9186 - val_loss: 0.3740 - val_acc: 0.8824\n",
            "Epoch 574/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2232 - acc: 0.9173 - val_loss: 0.3701 - val_acc: 0.8824\n",
            "Epoch 575/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2266 - acc: 0.9207 - val_loss: 0.3638 - val_acc: 0.8830\n",
            "Epoch 576/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2348 - acc: 0.9192 - val_loss: 0.3646 - val_acc: 0.8855\n",
            "Epoch 577/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2258 - acc: 0.9300 - val_loss: 0.3598 - val_acc: 0.8812\n",
            "Epoch 578/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2246 - acc: 0.9216 - val_loss: 0.3684 - val_acc: 0.8910\n",
            "Epoch 579/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2324 - acc: 0.9234 - val_loss: 0.3791 - val_acc: 0.8830\n",
            "Epoch 580/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2275 - acc: 0.9216 - val_loss: 0.4086 - val_acc: 0.8702\n",
            "Epoch 581/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2276 - acc: 0.9192 - val_loss: 0.3843 - val_acc: 0.8763\n",
            "Epoch 582/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2326 - acc: 0.9192 - val_loss: 0.3768 - val_acc: 0.8794\n",
            "Epoch 583/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2241 - acc: 0.9240 - val_loss: 0.3679 - val_acc: 0.8867\n",
            "Epoch 584/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2431 - acc: 0.9189 - val_loss: 0.3729 - val_acc: 0.8751\n",
            "Epoch 585/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2140 - acc: 0.9282 - val_loss: 0.3696 - val_acc: 0.8824\n",
            "Epoch 586/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2228 - acc: 0.9210 - val_loss: 0.3853 - val_acc: 0.8781\n",
            "Epoch 587/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2208 - acc: 0.9273 - val_loss: 0.3682 - val_acc: 0.8806\n",
            "Epoch 588/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2171 - acc: 0.9255 - val_loss: 0.3524 - val_acc: 0.8892\n",
            "Epoch 589/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2205 - acc: 0.9273 - val_loss: 0.3414 - val_acc: 0.8879\n",
            "Epoch 590/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2310 - acc: 0.9179 - val_loss: 0.3754 - val_acc: 0.8818\n",
            "Epoch 591/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2237 - acc: 0.9195 - val_loss: 0.3650 - val_acc: 0.8861\n",
            "Epoch 592/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2379 - acc: 0.9125 - val_loss: 0.3793 - val_acc: 0.8781\n",
            "Epoch 593/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2189 - acc: 0.9222 - val_loss: 0.3630 - val_acc: 0.8788\n",
            "Epoch 594/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2150 - acc: 0.9258 - val_loss: 0.3657 - val_acc: 0.8855\n",
            "Epoch 595/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.2213 - acc: 0.9258 - val_loss: 0.3799 - val_acc: 0.8745\n",
            "Epoch 596/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2152 - acc: 0.9273 - val_loss: 0.3699 - val_acc: 0.8904\n",
            "Epoch 597/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2127 - acc: 0.9261 - val_loss: 0.3817 - val_acc: 0.8757\n",
            "Epoch 598/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2136 - acc: 0.9291 - val_loss: 0.3835 - val_acc: 0.8757\n",
            "Epoch 599/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2167 - acc: 0.9258 - val_loss: 0.3611 - val_acc: 0.8904\n",
            "Epoch 600/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2076 - acc: 0.9342 - val_loss: 0.3617 - val_acc: 0.8818\n",
            "Epoch 601/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2358 - acc: 0.9189 - val_loss: 0.3562 - val_acc: 0.8861\n",
            "Epoch 602/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2100 - acc: 0.9291 - val_loss: 0.3647 - val_acc: 0.8873\n",
            "Epoch 603/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2180 - acc: 0.9279 - val_loss: 0.3678 - val_acc: 0.8763\n",
            "Epoch 604/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.3790 - val_acc: 0.8763\n",
            "Epoch 605/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2110 - acc: 0.9285 - val_loss: 0.3724 - val_acc: 0.8843\n",
            "Epoch 606/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2151 - acc: 0.9285 - val_loss: 0.3838 - val_acc: 0.8843\n",
            "Epoch 607/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1959 - acc: 0.9342 - val_loss: 0.3469 - val_acc: 0.8873\n",
            "Epoch 608/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2166 - acc: 0.9225 - val_loss: 0.3745 - val_acc: 0.8751\n",
            "Epoch 609/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2075 - acc: 0.9330 - val_loss: 0.3638 - val_acc: 0.8830\n",
            "Epoch 610/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2123 - acc: 0.9252 - val_loss: 0.3538 - val_acc: 0.8910\n",
            "Epoch 611/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2155 - acc: 0.9252 - val_loss: 0.3653 - val_acc: 0.8861\n",
            "Epoch 612/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2010 - acc: 0.9339 - val_loss: 0.3740 - val_acc: 0.8836\n",
            "Epoch 613/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2295 - acc: 0.9225 - val_loss: 0.3441 - val_acc: 0.9008\n",
            "Epoch 614/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2033 - acc: 0.9318 - val_loss: 0.3674 - val_acc: 0.8775\n",
            "Epoch 615/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2162 - acc: 0.9240 - val_loss: 0.3410 - val_acc: 0.9014\n",
            "Epoch 616/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1990 - acc: 0.9315 - val_loss: 0.3588 - val_acc: 0.8910\n",
            "Epoch 617/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2047 - acc: 0.9300 - val_loss: 0.3540 - val_acc: 0.8855\n",
            "Epoch 618/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2148 - acc: 0.9261 - val_loss: 0.3789 - val_acc: 0.8824\n",
            "Epoch 619/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2157 - acc: 0.9267 - val_loss: 0.3615 - val_acc: 0.8904\n",
            "Epoch 620/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2175 - acc: 0.9258 - val_loss: 0.3652 - val_acc: 0.8867\n",
            "Epoch 621/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2046 - acc: 0.9388 - val_loss: 0.3468 - val_acc: 0.8941\n",
            "Epoch 622/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2087 - acc: 0.9309 - val_loss: 0.3734 - val_acc: 0.8824\n",
            "Epoch 623/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2168 - acc: 0.9252 - val_loss: 0.3523 - val_acc: 0.8873\n",
            "Epoch 624/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2113 - acc: 0.9306 - val_loss: 0.3997 - val_acc: 0.8708\n",
            "Epoch 625/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2173 - acc: 0.9255 - val_loss: 0.3524 - val_acc: 0.8941\n",
            "Epoch 626/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2095 - acc: 0.9282 - val_loss: 0.3508 - val_acc: 0.8904\n",
            "Epoch 627/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.1902 - acc: 0.9376 - val_loss: 0.3527 - val_acc: 0.8904\n",
            "Epoch 628/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2125 - acc: 0.9276 - val_loss: 0.3565 - val_acc: 0.8879\n",
            "Epoch 629/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.1919 - acc: 0.9321 - val_loss: 0.3340 - val_acc: 0.8916\n",
            "Epoch 630/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2075 - acc: 0.9303 - val_loss: 0.3589 - val_acc: 0.8977\n",
            "Epoch 631/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2089 - acc: 0.9303 - val_loss: 0.3406 - val_acc: 0.8934\n",
            "Epoch 632/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2003 - acc: 0.9367 - val_loss: 0.3429 - val_acc: 0.8959\n",
            "Epoch 633/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2011 - acc: 0.9285 - val_loss: 0.3607 - val_acc: 0.8861\n",
            "Epoch 634/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2017 - acc: 0.9306 - val_loss: 0.3583 - val_acc: 0.8885\n",
            "Epoch 635/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2129 - acc: 0.9276 - val_loss: 0.3282 - val_acc: 0.8941\n",
            "Epoch 636/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2009 - acc: 0.9303 - val_loss: 0.3544 - val_acc: 0.8965\n",
            "Epoch 637/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2105 - acc: 0.9261 - val_loss: 0.3590 - val_acc: 0.8824\n",
            "Epoch 638/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1968 - acc: 0.9306 - val_loss: 0.3667 - val_acc: 0.8867\n",
            "Epoch 639/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2130 - acc: 0.9237 - val_loss: 0.3559 - val_acc: 0.8885\n",
            "Epoch 640/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2012 - acc: 0.9321 - val_loss: 0.3581 - val_acc: 0.8867\n",
            "Epoch 641/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2031 - acc: 0.9324 - val_loss: 0.3828 - val_acc: 0.8824\n",
            "Epoch 642/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1993 - acc: 0.9309 - val_loss: 0.3443 - val_acc: 0.8965\n",
            "Epoch 643/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.2061 - acc: 0.9309 - val_loss: 0.3485 - val_acc: 0.8922\n",
            "Epoch 644/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.2116 - acc: 0.9246 - val_loss: 0.3415 - val_acc: 0.8965\n",
            "Epoch 645/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1987 - acc: 0.9270 - val_loss: 0.3662 - val_acc: 0.8867\n",
            "Epoch 646/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1919 - acc: 0.9385 - val_loss: 0.3493 - val_acc: 0.8867\n",
            "Epoch 647/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1976 - acc: 0.9309 - val_loss: 0.3502 - val_acc: 0.8934\n",
            "Epoch 648/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.1881 - acc: 0.9336 - val_loss: 0.3778 - val_acc: 0.8849\n",
            "Epoch 649/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.1852 - acc: 0.9415 - val_loss: 0.3408 - val_acc: 0.8910\n",
            "Epoch 650/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.1826 - acc: 0.9373 - val_loss: 0.3537 - val_acc: 0.8867\n",
            "Epoch 651/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1945 - acc: 0.9339 - val_loss: 0.3809 - val_acc: 0.8824\n",
            "Epoch 652/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2005 - acc: 0.9276 - val_loss: 0.3455 - val_acc: 0.8922\n",
            "Epoch 653/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1967 - acc: 0.9318 - val_loss: 0.3449 - val_acc: 0.9014\n",
            "Epoch 654/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2101 - acc: 0.9282 - val_loss: 0.3408 - val_acc: 0.9026\n",
            "Epoch 655/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2079 - acc: 0.9279 - val_loss: 0.3343 - val_acc: 0.8983\n",
            "Epoch 656/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1959 - acc: 0.9321 - val_loss: 0.3381 - val_acc: 0.8983\n",
            "Epoch 657/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2130 - acc: 0.9276 - val_loss: 0.3614 - val_acc: 0.8892\n",
            "Epoch 658/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1930 - acc: 0.9342 - val_loss: 0.3575 - val_acc: 0.8910\n",
            "Epoch 659/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.1833 - acc: 0.9391 - val_loss: 0.3434 - val_acc: 0.8849\n",
            "Epoch 660/1000\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.1956 - acc: 0.9339 - val_loss: 0.3581 - val_acc: 0.8885\n",
            "Epoch 661/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1929 - acc: 0.9315 - val_loss: 0.3516 - val_acc: 0.8983\n",
            "Epoch 662/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2019 - acc: 0.9318 - val_loss: 0.3436 - val_acc: 0.8928\n",
            "Epoch 663/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1859 - acc: 0.9351 - val_loss: 0.3402 - val_acc: 0.8947\n",
            "Epoch 664/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1812 - acc: 0.9400 - val_loss: 0.3472 - val_acc: 0.8928\n",
            "Epoch 665/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1944 - acc: 0.9382 - val_loss: 0.3295 - val_acc: 0.9026\n",
            "Epoch 666/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1860 - acc: 0.9354 - val_loss: 0.3492 - val_acc: 0.8910\n",
            "Epoch 667/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1885 - acc: 0.9370 - val_loss: 0.3462 - val_acc: 0.8922\n",
            "Epoch 668/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1947 - acc: 0.9370 - val_loss: 0.3367 - val_acc: 0.9075\n",
            "Epoch 669/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1918 - acc: 0.9376 - val_loss: 0.3290 - val_acc: 0.9008\n",
            "Epoch 670/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1975 - acc: 0.9342 - val_loss: 0.3387 - val_acc: 0.8965\n",
            "Epoch 671/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1807 - acc: 0.9391 - val_loss: 0.3568 - val_acc: 0.8947\n",
            "Epoch 672/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1831 - acc: 0.9400 - val_loss: 0.3528 - val_acc: 0.8892\n",
            "Epoch 673/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1931 - acc: 0.9354 - val_loss: 0.3571 - val_acc: 0.8898\n",
            "Epoch 674/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1857 - acc: 0.9424 - val_loss: 0.3535 - val_acc: 0.8867\n",
            "Epoch 675/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1764 - acc: 0.9451 - val_loss: 0.3390 - val_acc: 0.8953\n",
            "Epoch 676/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1892 - acc: 0.9376 - val_loss: 0.3455 - val_acc: 0.8965\n",
            "Epoch 677/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1882 - acc: 0.9397 - val_loss: 0.3274 - val_acc: 0.8983\n",
            "Epoch 678/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1748 - acc: 0.9442 - val_loss: 0.3361 - val_acc: 0.9032\n",
            "Epoch 679/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1742 - acc: 0.9436 - val_loss: 0.3378 - val_acc: 0.9057\n",
            "Epoch 680/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1867 - acc: 0.9351 - val_loss: 0.3416 - val_acc: 0.9039\n",
            "Epoch 681/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.1841 - acc: 0.9336 - val_loss: 0.3276 - val_acc: 0.9039\n",
            "Epoch 682/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.1907 - acc: 0.9357 - val_loss: 0.3437 - val_acc: 0.9020\n",
            "Epoch 683/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1868 - acc: 0.9409 - val_loss: 0.3248 - val_acc: 0.9008\n",
            "Epoch 684/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1764 - acc: 0.9382 - val_loss: 0.3384 - val_acc: 0.8977\n",
            "Epoch 685/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1795 - acc: 0.9345 - val_loss: 0.3480 - val_acc: 0.8904\n",
            "Epoch 686/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1866 - acc: 0.9318 - val_loss: 0.3518 - val_acc: 0.8990\n",
            "Epoch 687/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1776 - acc: 0.9403 - val_loss: 0.3493 - val_acc: 0.8971\n",
            "Epoch 688/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1906 - acc: 0.9348 - val_loss: 0.3470 - val_acc: 0.8885\n",
            "Epoch 689/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.1754 - acc: 0.9397 - val_loss: 0.3273 - val_acc: 0.9045\n",
            "Epoch 690/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.1835 - acc: 0.9382 - val_loss: 0.3310 - val_acc: 0.9020\n",
            "Epoch 691/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1840 - acc: 0.9388 - val_loss: 0.3418 - val_acc: 0.9002\n",
            "Epoch 692/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1800 - acc: 0.9394 - val_loss: 0.3477 - val_acc: 0.9002\n",
            "Epoch 693/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1891 - acc: 0.9418 - val_loss: 0.3416 - val_acc: 0.8941\n",
            "Epoch 694/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1819 - acc: 0.9373 - val_loss: 0.3278 - val_acc: 0.8953\n",
            "Epoch 695/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1834 - acc: 0.9382 - val_loss: 0.3343 - val_acc: 0.8941\n",
            "Epoch 696/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1833 - acc: 0.9412 - val_loss: 0.3486 - val_acc: 0.8996\n",
            "Epoch 697/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1750 - acc: 0.9442 - val_loss: 0.3354 - val_acc: 0.8996\n",
            "Epoch 698/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1712 - acc: 0.9421 - val_loss: 0.3210 - val_acc: 0.9002\n",
            "Epoch 699/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1816 - acc: 0.9388 - val_loss: 0.3200 - val_acc: 0.9045\n",
            "Epoch 700/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.1797 - acc: 0.9400 - val_loss: 0.3371 - val_acc: 0.8959\n",
            "Epoch 701/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1727 - acc: 0.9357 - val_loss: 0.3625 - val_acc: 0.8898\n",
            "Epoch 702/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1816 - acc: 0.9363 - val_loss: 0.3274 - val_acc: 0.8959\n",
            "Epoch 703/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1753 - acc: 0.9427 - val_loss: 0.3261 - val_acc: 0.9026\n",
            "Epoch 704/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1864 - acc: 0.9382 - val_loss: 0.3517 - val_acc: 0.8904\n",
            "Epoch 705/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1911 - acc: 0.9373 - val_loss: 0.3392 - val_acc: 0.8965\n",
            "Epoch 706/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1838 - acc: 0.9360 - val_loss: 0.3361 - val_acc: 0.9014\n",
            "Epoch 707/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.1648 - acc: 0.9397 - val_loss: 0.3478 - val_acc: 0.8990\n",
            "Epoch 708/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1792 - acc: 0.9421 - val_loss: 0.3496 - val_acc: 0.8977\n",
            "Epoch 709/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1838 - acc: 0.9363 - val_loss: 0.3249 - val_acc: 0.9045\n",
            "Epoch 710/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1864 - acc: 0.9348 - val_loss: 0.3328 - val_acc: 0.9051\n",
            "Epoch 711/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1744 - acc: 0.9379 - val_loss: 0.3103 - val_acc: 0.9094\n",
            "Epoch 712/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1764 - acc: 0.9409 - val_loss: 0.3433 - val_acc: 0.8990\n",
            "Epoch 713/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1841 - acc: 0.9370 - val_loss: 0.3281 - val_acc: 0.9014\n",
            "Epoch 714/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1689 - acc: 0.9439 - val_loss: 0.3336 - val_acc: 0.8922\n",
            "Epoch 715/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.1672 - acc: 0.9442 - val_loss: 0.3338 - val_acc: 0.9026\n",
            "Epoch 716/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1696 - acc: 0.9412 - val_loss: 0.3375 - val_acc: 0.8977\n",
            "Epoch 717/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1721 - acc: 0.9385 - val_loss: 0.3364 - val_acc: 0.9057\n",
            "Epoch 718/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1607 - acc: 0.9496 - val_loss: 0.3409 - val_acc: 0.9002\n",
            "Epoch 719/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1791 - acc: 0.9376 - val_loss: 0.3308 - val_acc: 0.8990\n",
            "Epoch 720/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1676 - acc: 0.9430 - val_loss: 0.3260 - val_acc: 0.9051\n",
            "Epoch 721/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1630 - acc: 0.9454 - val_loss: 0.3315 - val_acc: 0.9014\n",
            "Epoch 722/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1799 - acc: 0.9363 - val_loss: 0.3417 - val_acc: 0.9002\n",
            "Epoch 723/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1683 - acc: 0.9412 - val_loss: 0.3361 - val_acc: 0.8910\n",
            "Epoch 724/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1683 - acc: 0.9394 - val_loss: 0.3309 - val_acc: 0.8990\n",
            "Epoch 725/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1786 - acc: 0.9354 - val_loss: 0.3417 - val_acc: 0.9008\n",
            "Epoch 726/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1709 - acc: 0.9448 - val_loss: 0.3541 - val_acc: 0.8983\n",
            "Epoch 727/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1584 - acc: 0.9475 - val_loss: 0.3374 - val_acc: 0.8996\n",
            "Epoch 728/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1718 - acc: 0.9463 - val_loss: 0.3430 - val_acc: 0.8977\n",
            "Epoch 729/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.1687 - acc: 0.9421 - val_loss: 0.3511 - val_acc: 0.8898\n",
            "Epoch 730/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1706 - acc: 0.9466 - val_loss: 0.3622 - val_acc: 0.8916\n",
            "Epoch 731/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1756 - acc: 0.9421 - val_loss: 0.3365 - val_acc: 0.8996\n",
            "Epoch 732/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1679 - acc: 0.9400 - val_loss: 0.3311 - val_acc: 0.9069\n",
            "Epoch 733/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1846 - acc: 0.9370 - val_loss: 0.3150 - val_acc: 0.9088\n",
            "Epoch 734/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1668 - acc: 0.9400 - val_loss: 0.3440 - val_acc: 0.8959\n",
            "Epoch 735/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1558 - acc: 0.9460 - val_loss: 0.3431 - val_acc: 0.8922\n",
            "Epoch 736/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1818 - acc: 0.9376 - val_loss: 0.3280 - val_acc: 0.8990\n",
            "Epoch 737/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1703 - acc: 0.9439 - val_loss: 0.3511 - val_acc: 0.9002\n",
            "Epoch 738/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1693 - acc: 0.9451 - val_loss: 0.3426 - val_acc: 0.8996\n",
            "Epoch 739/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1625 - acc: 0.9412 - val_loss: 0.3262 - val_acc: 0.9063\n",
            "Epoch 740/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1756 - acc: 0.9430 - val_loss: 0.3433 - val_acc: 0.8983\n",
            "Epoch 741/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1687 - acc: 0.9427 - val_loss: 0.3305 - val_acc: 0.9137\n",
            "Epoch 742/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1603 - acc: 0.9466 - val_loss: 0.3407 - val_acc: 0.9008\n",
            "Epoch 743/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.1750 - acc: 0.9412 - val_loss: 0.3399 - val_acc: 0.8996\n",
            "Epoch 744/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1547 - acc: 0.9496 - val_loss: 0.3570 - val_acc: 0.8941\n",
            "Epoch 745/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1604 - acc: 0.9427 - val_loss: 0.3238 - val_acc: 0.9057\n",
            "Epoch 746/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.1643 - acc: 0.9460 - val_loss: 0.3320 - val_acc: 0.9008\n",
            "Epoch 747/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1666 - acc: 0.9445 - val_loss: 0.3470 - val_acc: 0.8971\n",
            "Epoch 748/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1717 - acc: 0.9421 - val_loss: 0.3335 - val_acc: 0.8971\n",
            "Epoch 749/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1501 - acc: 0.9538 - val_loss: 0.3183 - val_acc: 0.9069\n",
            "Epoch 750/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1599 - acc: 0.9427 - val_loss: 0.3240 - val_acc: 0.8990\n",
            "Epoch 751/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1601 - acc: 0.9487 - val_loss: 0.3317 - val_acc: 0.8947\n",
            "Epoch 752/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1642 - acc: 0.9424 - val_loss: 0.3142 - val_acc: 0.9081\n",
            "Epoch 753/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1598 - acc: 0.9466 - val_loss: 0.3027 - val_acc: 0.9081\n",
            "Epoch 754/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1500 - acc: 0.9499 - val_loss: 0.3181 - val_acc: 0.9051\n",
            "Epoch 755/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1525 - acc: 0.9544 - val_loss: 0.3371 - val_acc: 0.9014\n",
            "Epoch 756/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1629 - acc: 0.9466 - val_loss: 0.3239 - val_acc: 0.9094\n",
            "Epoch 757/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1640 - acc: 0.9415 - val_loss: 0.3213 - val_acc: 0.9057\n",
            "Epoch 758/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1590 - acc: 0.9487 - val_loss: 0.3256 - val_acc: 0.9069\n",
            "Epoch 759/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1571 - acc: 0.9469 - val_loss: 0.3257 - val_acc: 0.9002\n",
            "Epoch 760/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1552 - acc: 0.9481 - val_loss: 0.3393 - val_acc: 0.9020\n",
            "Epoch 761/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1571 - acc: 0.9445 - val_loss: 0.3387 - val_acc: 0.9069\n",
            "Epoch 762/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1561 - acc: 0.9493 - val_loss: 0.3423 - val_acc: 0.8947\n",
            "Epoch 763/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1583 - acc: 0.9475 - val_loss: 0.3356 - val_acc: 0.8953\n",
            "Epoch 764/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1586 - acc: 0.9478 - val_loss: 0.3190 - val_acc: 0.9143\n",
            "Epoch 765/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1633 - acc: 0.9454 - val_loss: 0.3214 - val_acc: 0.9088\n",
            "Epoch 766/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1648 - acc: 0.9448 - val_loss: 0.3220 - val_acc: 0.9106\n",
            "Epoch 767/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1513 - acc: 0.9517 - val_loss: 0.3393 - val_acc: 0.8904\n",
            "Epoch 768/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1585 - acc: 0.9490 - val_loss: 0.3173 - val_acc: 0.9039\n",
            "Epoch 769/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1613 - acc: 0.9433 - val_loss: 0.3156 - val_acc: 0.9069\n",
            "Epoch 770/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1544 - acc: 0.9511 - val_loss: 0.3311 - val_acc: 0.9057\n",
            "Epoch 771/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1595 - acc: 0.9433 - val_loss: 0.3322 - val_acc: 0.8971\n",
            "Epoch 772/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1600 - acc: 0.9424 - val_loss: 0.3573 - val_acc: 0.8922\n",
            "Epoch 773/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1551 - acc: 0.9445 - val_loss: 0.3408 - val_acc: 0.8953\n",
            "Epoch 774/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1620 - acc: 0.9436 - val_loss: 0.3151 - val_acc: 0.9032\n",
            "Epoch 775/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1540 - acc: 0.9490 - val_loss: 0.3513 - val_acc: 0.8904\n",
            "Epoch 776/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1623 - acc: 0.9469 - val_loss: 0.3180 - val_acc: 0.9075\n",
            "Epoch 777/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1575 - acc: 0.9424 - val_loss: 0.3589 - val_acc: 0.8904\n",
            "Epoch 778/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1502 - acc: 0.9466 - val_loss: 0.3449 - val_acc: 0.9045\n",
            "Epoch 779/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1550 - acc: 0.9475 - val_loss: 0.3202 - val_acc: 0.9057\n",
            "Epoch 780/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1537 - acc: 0.9484 - val_loss: 0.3159 - val_acc: 0.9063\n",
            "Epoch 781/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1662 - acc: 0.9433 - val_loss: 0.3055 - val_acc: 0.9063\n",
            "Epoch 782/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1626 - acc: 0.9484 - val_loss: 0.3273 - val_acc: 0.9008\n",
            "Epoch 783/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1520 - acc: 0.9484 - val_loss: 0.3295 - val_acc: 0.8996\n",
            "Epoch 784/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1450 - acc: 0.9526 - val_loss: 0.3196 - val_acc: 0.9032\n",
            "Epoch 785/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1469 - acc: 0.9487 - val_loss: 0.3666 - val_acc: 0.9002\n",
            "Epoch 786/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1444 - acc: 0.9487 - val_loss: 0.3411 - val_acc: 0.9020\n",
            "Epoch 787/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1512 - acc: 0.9463 - val_loss: 0.3247 - val_acc: 0.9039\n",
            "Epoch 788/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1452 - acc: 0.9505 - val_loss: 0.3075 - val_acc: 0.9026\n",
            "Epoch 789/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1419 - acc: 0.9535 - val_loss: 0.3078 - val_acc: 0.9057\n",
            "Epoch 790/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1411 - acc: 0.9511 - val_loss: 0.3495 - val_acc: 0.8885\n",
            "Epoch 791/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1523 - acc: 0.9517 - val_loss: 0.3230 - val_acc: 0.9032\n",
            "Epoch 792/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1558 - acc: 0.9466 - val_loss: 0.3256 - val_acc: 0.8996\n",
            "Epoch 793/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1495 - acc: 0.9499 - val_loss: 0.3305 - val_acc: 0.9020\n",
            "Epoch 794/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1629 - acc: 0.9415 - val_loss: 0.3162 - val_acc: 0.9063\n",
            "Epoch 795/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1518 - acc: 0.9490 - val_loss: 0.3227 - val_acc: 0.9069\n",
            "Epoch 796/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1505 - acc: 0.9499 - val_loss: 0.3413 - val_acc: 0.8996\n",
            "Epoch 797/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1428 - acc: 0.9532 - val_loss: 0.3277 - val_acc: 0.9051\n",
            "Epoch 798/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1448 - acc: 0.9502 - val_loss: 0.3313 - val_acc: 0.9008\n",
            "Epoch 799/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1441 - acc: 0.9517 - val_loss: 0.3094 - val_acc: 0.9112\n",
            "Epoch 800/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1644 - acc: 0.9442 - val_loss: 0.3085 - val_acc: 0.9100\n",
            "Epoch 801/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1462 - acc: 0.9472 - val_loss: 0.3147 - val_acc: 0.9088\n",
            "Epoch 802/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1527 - acc: 0.9517 - val_loss: 0.3124 - val_acc: 0.9069\n",
            "Epoch 803/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1529 - acc: 0.9439 - val_loss: 0.3191 - val_acc: 0.9032\n",
            "Epoch 804/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1478 - acc: 0.9508 - val_loss: 0.3320 - val_acc: 0.8977\n",
            "Epoch 805/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1397 - acc: 0.9520 - val_loss: 0.3666 - val_acc: 0.8904\n",
            "Epoch 806/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1385 - acc: 0.9511 - val_loss: 0.3364 - val_acc: 0.8996\n",
            "Epoch 807/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1482 - acc: 0.9484 - val_loss: 0.3160 - val_acc: 0.9039\n",
            "Epoch 808/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1491 - acc: 0.9517 - val_loss: 0.3249 - val_acc: 0.9045\n",
            "Epoch 809/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1312 - acc: 0.9560 - val_loss: 0.3314 - val_acc: 0.9069\n",
            "Epoch 810/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1412 - acc: 0.9481 - val_loss: 0.3509 - val_acc: 0.8996\n",
            "Epoch 811/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1557 - acc: 0.9478 - val_loss: 0.3358 - val_acc: 0.8971\n",
            "Epoch 812/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1590 - acc: 0.9442 - val_loss: 0.3402 - val_acc: 0.8965\n",
            "Epoch 813/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1408 - acc: 0.9526 - val_loss: 0.3389 - val_acc: 0.9039\n",
            "Epoch 814/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1385 - acc: 0.9554 - val_loss: 0.3423 - val_acc: 0.8990\n",
            "Epoch 815/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1557 - acc: 0.9472 - val_loss: 0.3017 - val_acc: 0.9124\n",
            "Epoch 816/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1438 - acc: 0.9554 - val_loss: 0.3381 - val_acc: 0.8953\n",
            "Epoch 817/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1534 - acc: 0.9466 - val_loss: 0.3156 - val_acc: 0.9057\n",
            "Epoch 818/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1435 - acc: 0.9541 - val_loss: 0.3457 - val_acc: 0.9039\n",
            "Epoch 819/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1423 - acc: 0.9575 - val_loss: 0.3178 - val_acc: 0.9088\n",
            "Epoch 820/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1296 - acc: 0.9523 - val_loss: 0.3411 - val_acc: 0.9057\n",
            "Epoch 821/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1544 - acc: 0.9460 - val_loss: 0.3337 - val_acc: 0.9051\n",
            "Epoch 822/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1320 - acc: 0.9569 - val_loss: 0.3193 - val_acc: 0.9106\n",
            "Epoch 823/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1436 - acc: 0.9535 - val_loss: 0.3067 - val_acc: 0.9081\n",
            "Epoch 824/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1384 - acc: 0.9538 - val_loss: 0.3303 - val_acc: 0.9094\n",
            "Epoch 825/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1456 - acc: 0.9499 - val_loss: 0.3479 - val_acc: 0.8928\n",
            "Epoch 826/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1450 - acc: 0.9578 - val_loss: 0.3504 - val_acc: 0.8977\n",
            "Epoch 827/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1418 - acc: 0.9481 - val_loss: 0.3198 - val_acc: 0.9106\n",
            "Epoch 828/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1438 - acc: 0.9514 - val_loss: 0.3299 - val_acc: 0.8959\n",
            "Epoch 829/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1364 - acc: 0.9544 - val_loss: 0.3407 - val_acc: 0.8971\n",
            "Epoch 830/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1462 - acc: 0.9484 - val_loss: 0.3346 - val_acc: 0.9069\n",
            "Epoch 831/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1370 - acc: 0.9532 - val_loss: 0.3315 - val_acc: 0.9045\n",
            "Epoch 832/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1331 - acc: 0.9514 - val_loss: 0.3131 - val_acc: 0.9069\n",
            "Epoch 833/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1467 - acc: 0.9484 - val_loss: 0.3139 - val_acc: 0.9081\n",
            "Epoch 834/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1342 - acc: 0.9535 - val_loss: 0.3431 - val_acc: 0.8977\n",
            "Epoch 835/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1485 - acc: 0.9475 - val_loss: 0.3101 - val_acc: 0.9088\n",
            "Epoch 836/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1273 - acc: 0.9548 - val_loss: 0.3179 - val_acc: 0.9094\n",
            "Epoch 837/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1371 - acc: 0.9587 - val_loss: 0.3033 - val_acc: 0.9088\n",
            "Epoch 838/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1240 - acc: 0.9551 - val_loss: 0.3202 - val_acc: 0.9014\n",
            "Epoch 839/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1415 - acc: 0.9472 - val_loss: 0.3405 - val_acc: 0.8977\n",
            "Epoch 840/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1414 - acc: 0.9529 - val_loss: 0.3240 - val_acc: 0.9008\n",
            "Epoch 841/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1429 - acc: 0.9532 - val_loss: 0.3292 - val_acc: 0.9063\n",
            "Epoch 842/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1450 - acc: 0.9493 - val_loss: 0.3164 - val_acc: 0.9020\n",
            "Epoch 843/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1446 - acc: 0.9517 - val_loss: 0.3483 - val_acc: 0.9014\n",
            "Epoch 844/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1338 - acc: 0.9538 - val_loss: 0.3324 - val_acc: 0.9032\n",
            "Epoch 845/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1424 - acc: 0.9517 - val_loss: 0.3263 - val_acc: 0.9008\n",
            "Epoch 846/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1370 - acc: 0.9544 - val_loss: 0.3249 - val_acc: 0.9051\n",
            "Epoch 847/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1464 - acc: 0.9487 - val_loss: 0.3119 - val_acc: 0.9192\n",
            "Epoch 848/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1361 - acc: 0.9541 - val_loss: 0.3301 - val_acc: 0.9118\n",
            "Epoch 849/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1446 - acc: 0.9469 - val_loss: 0.3365 - val_acc: 0.9075\n",
            "Epoch 850/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1238 - acc: 0.9611 - val_loss: 0.3509 - val_acc: 0.8996\n",
            "Epoch 851/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1456 - acc: 0.9535 - val_loss: 0.3283 - val_acc: 0.9075\n",
            "Epoch 852/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1201 - acc: 0.9578 - val_loss: 0.3401 - val_acc: 0.9039\n",
            "Epoch 853/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1346 - acc: 0.9535 - val_loss: 0.3209 - val_acc: 0.9063\n",
            "Epoch 854/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1431 - acc: 0.9569 - val_loss: 0.3313 - val_acc: 0.8983\n",
            "Epoch 855/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1277 - acc: 0.9599 - val_loss: 0.3478 - val_acc: 0.8971\n",
            "Epoch 856/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1358 - acc: 0.9520 - val_loss: 0.3124 - val_acc: 0.9118\n",
            "Epoch 857/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1515 - acc: 0.9505 - val_loss: 0.3267 - val_acc: 0.9069\n",
            "Epoch 858/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1333 - acc: 0.9563 - val_loss: 0.3190 - val_acc: 0.9112\n",
            "Epoch 859/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1268 - acc: 0.9569 - val_loss: 0.3295 - val_acc: 0.9045\n",
            "Epoch 860/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1298 - acc: 0.9548 - val_loss: 0.3042 - val_acc: 0.9179\n",
            "Epoch 861/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1393 - acc: 0.9535 - val_loss: 0.3282 - val_acc: 0.9008\n",
            "Epoch 862/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1350 - acc: 0.9551 - val_loss: 0.3280 - val_acc: 0.9112\n",
            "Epoch 863/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1401 - acc: 0.9523 - val_loss: 0.3420 - val_acc: 0.9002\n",
            "Epoch 864/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1236 - acc: 0.9566 - val_loss: 0.3320 - val_acc: 0.8983\n",
            "Epoch 865/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1242 - acc: 0.9587 - val_loss: 0.3185 - val_acc: 0.9081\n",
            "Epoch 866/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1412 - acc: 0.9532 - val_loss: 0.3155 - val_acc: 0.9075\n",
            "Epoch 867/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1342 - acc: 0.9587 - val_loss: 0.3204 - val_acc: 0.9075\n",
            "Epoch 868/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1362 - acc: 0.9517 - val_loss: 0.3070 - val_acc: 0.9063\n",
            "Epoch 869/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1244 - acc: 0.9566 - val_loss: 0.3484 - val_acc: 0.9020\n",
            "Epoch 870/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1430 - acc: 0.9532 - val_loss: 0.3298 - val_acc: 0.9069\n",
            "Epoch 871/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1386 - acc: 0.9523 - val_loss: 0.3401 - val_acc: 0.9008\n",
            "Epoch 872/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1241 - acc: 0.9535 - val_loss: 0.3093 - val_acc: 0.9167\n",
            "Epoch 873/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1267 - acc: 0.9578 - val_loss: 0.3192 - val_acc: 0.9155\n",
            "Epoch 874/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1371 - acc: 0.9560 - val_loss: 0.3120 - val_acc: 0.9045\n",
            "Epoch 875/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1344 - acc: 0.9532 - val_loss: 0.3242 - val_acc: 0.9026\n",
            "Epoch 876/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1407 - acc: 0.9529 - val_loss: 0.3245 - val_acc: 0.9167\n",
            "Epoch 877/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1309 - acc: 0.9511 - val_loss: 0.3346 - val_acc: 0.9008\n",
            "Epoch 878/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1204 - acc: 0.9614 - val_loss: 0.3119 - val_acc: 0.9051\n",
            "Epoch 879/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3277 - val_acc: 0.9069\n",
            "Epoch 880/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1414 - acc: 0.9541 - val_loss: 0.3359 - val_acc: 0.9081\n",
            "Epoch 881/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1202 - acc: 0.9590 - val_loss: 0.3226 - val_acc: 0.9051\n",
            "Epoch 882/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1297 - acc: 0.9544 - val_loss: 0.3302 - val_acc: 0.8996\n",
            "Epoch 883/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1343 - acc: 0.9472 - val_loss: 0.3386 - val_acc: 0.9069\n",
            "Epoch 884/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.1191 - acc: 0.9566 - val_loss: 0.3091 - val_acc: 0.9124\n",
            "Epoch 885/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1360 - acc: 0.9523 - val_loss: 0.3394 - val_acc: 0.9094\n",
            "Epoch 886/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1225 - acc: 0.9581 - val_loss: 0.3128 - val_acc: 0.9057\n",
            "Epoch 887/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1407 - acc: 0.9572 - val_loss: 0.3042 - val_acc: 0.9149\n",
            "Epoch 888/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1274 - acc: 0.9541 - val_loss: 0.3370 - val_acc: 0.9002\n",
            "Epoch 889/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1235 - acc: 0.9575 - val_loss: 0.3249 - val_acc: 0.9014\n",
            "Epoch 890/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1307 - acc: 0.9557 - val_loss: 0.3217 - val_acc: 0.9032\n",
            "Epoch 891/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1154 - acc: 0.9608 - val_loss: 0.3245 - val_acc: 0.9088\n",
            "Epoch 892/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1267 - acc: 0.9569 - val_loss: 0.3213 - val_acc: 0.9081\n",
            "Epoch 893/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.1436 - acc: 0.9511 - val_loss: 0.3568 - val_acc: 0.9057\n",
            "Epoch 894/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1345 - acc: 0.9563 - val_loss: 0.3201 - val_acc: 0.9081\n",
            "Epoch 895/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1279 - acc: 0.9560 - val_loss: 0.3553 - val_acc: 0.9081\n",
            "Epoch 896/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1335 - acc: 0.9578 - val_loss: 0.3204 - val_acc: 0.9112\n",
            "Epoch 897/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.1333 - acc: 0.9563 - val_loss: 0.3274 - val_acc: 0.9130\n",
            "Epoch 898/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1250 - acc: 0.9596 - val_loss: 0.3251 - val_acc: 0.9130\n",
            "Epoch 899/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1154 - acc: 0.9608 - val_loss: 0.3168 - val_acc: 0.9130\n",
            "Epoch 900/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1198 - acc: 0.9626 - val_loss: 0.3085 - val_acc: 0.9167\n",
            "Epoch 901/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1301 - acc: 0.9563 - val_loss: 0.3296 - val_acc: 0.9081\n",
            "Epoch 902/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1357 - acc: 0.9566 - val_loss: 0.3331 - val_acc: 0.9088\n",
            "Epoch 903/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1304 - acc: 0.9569 - val_loss: 0.3093 - val_acc: 0.9130\n",
            "Epoch 904/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1170 - acc: 0.9617 - val_loss: 0.3278 - val_acc: 0.9118\n",
            "Epoch 905/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1266 - acc: 0.9578 - val_loss: 0.2956 - val_acc: 0.9130\n",
            "Epoch 906/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1196 - acc: 0.9605 - val_loss: 0.3262 - val_acc: 0.9106\n",
            "Epoch 907/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1201 - acc: 0.9587 - val_loss: 0.3156 - val_acc: 0.9100\n",
            "Epoch 908/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1203 - acc: 0.9602 - val_loss: 0.3382 - val_acc: 0.9124\n",
            "Epoch 909/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1281 - acc: 0.9578 - val_loss: 0.3241 - val_acc: 0.9032\n",
            "Epoch 910/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1330 - acc: 0.9560 - val_loss: 0.3265 - val_acc: 0.9063\n",
            "Epoch 911/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1321 - acc: 0.9532 - val_loss: 0.3480 - val_acc: 0.9020\n",
            "Epoch 912/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1324 - acc: 0.9632 - val_loss: 0.3406 - val_acc: 0.9075\n",
            "Epoch 913/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1195 - acc: 0.9596 - val_loss: 0.3278 - val_acc: 0.9002\n",
            "Epoch 914/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1224 - acc: 0.9596 - val_loss: 0.3192 - val_acc: 0.9124\n",
            "Epoch 915/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1132 - acc: 0.9617 - val_loss: 0.3060 - val_acc: 0.9081\n",
            "Epoch 916/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1165 - acc: 0.9617 - val_loss: 0.3208 - val_acc: 0.9075\n",
            "Epoch 917/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1267 - acc: 0.9575 - val_loss: 0.3110 - val_acc: 0.9137\n",
            "Epoch 918/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1154 - acc: 0.9623 - val_loss: 0.3041 - val_acc: 0.9155\n",
            "Epoch 919/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1241 - acc: 0.9584 - val_loss: 0.3270 - val_acc: 0.9081\n",
            "Epoch 920/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1322 - acc: 0.9572 - val_loss: 0.2985 - val_acc: 0.9155\n",
            "Epoch 921/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1228 - acc: 0.9581 - val_loss: 0.3257 - val_acc: 0.9081\n",
            "Epoch 922/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1096 - acc: 0.9629 - val_loss: 0.3016 - val_acc: 0.9149\n",
            "Epoch 923/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1296 - acc: 0.9505 - val_loss: 0.3331 - val_acc: 0.9075\n",
            "Epoch 924/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.1226 - acc: 0.9599 - val_loss: 0.3274 - val_acc: 0.9075\n",
            "Epoch 925/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1157 - acc: 0.9605 - val_loss: 0.3574 - val_acc: 0.9045\n",
            "Epoch 926/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1350 - acc: 0.9566 - val_loss: 0.3355 - val_acc: 0.9081\n",
            "Epoch 927/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1238 - acc: 0.9593 - val_loss: 0.3268 - val_acc: 0.9130\n",
            "Epoch 928/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1171 - acc: 0.9620 - val_loss: 0.3233 - val_acc: 0.9081\n",
            "Epoch 929/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1182 - acc: 0.9620 - val_loss: 0.3448 - val_acc: 0.9124\n",
            "Epoch 930/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1176 - acc: 0.9608 - val_loss: 0.3162 - val_acc: 0.9100\n",
            "Epoch 931/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1264 - acc: 0.9581 - val_loss: 0.3093 - val_acc: 0.9100\n",
            "Epoch 932/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1235 - acc: 0.9587 - val_loss: 0.3269 - val_acc: 0.9100\n",
            "Epoch 933/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1139 - acc: 0.9596 - val_loss: 0.3242 - val_acc: 0.9094\n",
            "Epoch 934/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1214 - acc: 0.9566 - val_loss: 0.3187 - val_acc: 0.9118\n",
            "Epoch 935/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1156 - acc: 0.9632 - val_loss: 0.3135 - val_acc: 0.9088\n",
            "Epoch 936/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1303 - acc: 0.9529 - val_loss: 0.3067 - val_acc: 0.9137\n",
            "Epoch 937/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.1185 - acc: 0.9611 - val_loss: 0.3546 - val_acc: 0.8996\n",
            "Epoch 938/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.1232 - acc: 0.9548 - val_loss: 0.3065 - val_acc: 0.9149\n",
            "Epoch 939/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.1114 - acc: 0.9602 - val_loss: 0.3135 - val_acc: 0.9112\n",
            "Epoch 940/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1266 - acc: 0.9593 - val_loss: 0.3114 - val_acc: 0.9161\n",
            "Epoch 941/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1172 - acc: 0.9632 - val_loss: 0.3172 - val_acc: 0.9100\n",
            "Epoch 942/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1243 - acc: 0.9584 - val_loss: 0.3248 - val_acc: 0.9118\n",
            "Epoch 943/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1171 - acc: 0.9599 - val_loss: 0.3099 - val_acc: 0.9143\n",
            "Epoch 944/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1352 - acc: 0.9569 - val_loss: 0.3032 - val_acc: 0.9173\n",
            "Epoch 945/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1174 - acc: 0.9599 - val_loss: 0.3253 - val_acc: 0.9075\n",
            "Epoch 946/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1126 - acc: 0.9602 - val_loss: 0.3242 - val_acc: 0.9081\n",
            "Epoch 947/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1309 - acc: 0.9526 - val_loss: 0.3004 - val_acc: 0.9143\n",
            "Epoch 948/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1199 - acc: 0.9596 - val_loss: 0.3107 - val_acc: 0.9057\n",
            "Epoch 949/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1124 - acc: 0.9626 - val_loss: 0.3098 - val_acc: 0.9039\n",
            "Epoch 950/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1168 - acc: 0.9620 - val_loss: 0.3048 - val_acc: 0.9106\n",
            "Epoch 951/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1284 - acc: 0.9599 - val_loss: 0.3059 - val_acc: 0.9112\n",
            "Epoch 952/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1155 - acc: 0.9629 - val_loss: 0.3183 - val_acc: 0.9161\n",
            "Epoch 953/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1291 - acc: 0.9560 - val_loss: 0.2999 - val_acc: 0.9161\n",
            "Epoch 954/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1192 - acc: 0.9584 - val_loss: 0.3369 - val_acc: 0.9014\n",
            "Epoch 955/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1149 - acc: 0.9620 - val_loss: 0.3064 - val_acc: 0.9124\n",
            "Epoch 956/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1147 - acc: 0.9629 - val_loss: 0.3111 - val_acc: 0.9106\n",
            "Epoch 957/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1084 - acc: 0.9608 - val_loss: 0.3137 - val_acc: 0.9069\n",
            "Epoch 958/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1064 - acc: 0.9647 - val_loss: 0.3377 - val_acc: 0.9051\n",
            "Epoch 959/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1287 - acc: 0.9557 - val_loss: 0.3126 - val_acc: 0.9112\n",
            "Epoch 960/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1221 - acc: 0.9569 - val_loss: 0.3181 - val_acc: 0.9081\n",
            "Epoch 961/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1017 - acc: 0.9638 - val_loss: 0.3180 - val_acc: 0.9106\n",
            "Epoch 962/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1104 - acc: 0.9599 - val_loss: 0.2886 - val_acc: 0.9186\n",
            "Epoch 963/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1367 - acc: 0.9578 - val_loss: 0.3016 - val_acc: 0.9198\n",
            "Epoch 964/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1270 - acc: 0.9611 - val_loss: 0.2966 - val_acc: 0.9222\n",
            "Epoch 965/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.1210 - acc: 0.9572 - val_loss: 0.3100 - val_acc: 0.9118\n",
            "Epoch 966/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1138 - acc: 0.9581 - val_loss: 0.3469 - val_acc: 0.9032\n",
            "Epoch 967/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1194 - acc: 0.9599 - val_loss: 0.3358 - val_acc: 0.9118\n",
            "Epoch 968/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1071 - acc: 0.9620 - val_loss: 0.3263 - val_acc: 0.9063\n",
            "Epoch 969/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1177 - acc: 0.9587 - val_loss: 0.3164 - val_acc: 0.9094\n",
            "Epoch 970/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1049 - acc: 0.9644 - val_loss: 0.3308 - val_acc: 0.9143\n",
            "Epoch 971/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1075 - acc: 0.9647 - val_loss: 0.3212 - val_acc: 0.9057\n",
            "Epoch 972/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1181 - acc: 0.9611 - val_loss: 0.3337 - val_acc: 0.9063\n",
            "Epoch 973/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1094 - acc: 0.9611 - val_loss: 0.3341 - val_acc: 0.9081\n",
            "Epoch 974/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1229 - acc: 0.9602 - val_loss: 0.3340 - val_acc: 0.9026\n",
            "Epoch 975/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1187 - acc: 0.9593 - val_loss: 0.3356 - val_acc: 0.9124\n",
            "Epoch 976/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1304 - acc: 0.9544 - val_loss: 0.3273 - val_acc: 0.9124\n",
            "Epoch 977/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1202 - acc: 0.9581 - val_loss: 0.3165 - val_acc: 0.9130\n",
            "Epoch 978/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1034 - acc: 0.9695 - val_loss: 0.3060 - val_acc: 0.9118\n",
            "Epoch 979/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1195 - acc: 0.9608 - val_loss: 0.3205 - val_acc: 0.9130\n",
            "Epoch 980/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1108 - acc: 0.9605 - val_loss: 0.3116 - val_acc: 0.9112\n",
            "Epoch 981/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1187 - acc: 0.9608 - val_loss: 0.3361 - val_acc: 0.9167\n",
            "Epoch 982/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1119 - acc: 0.9611 - val_loss: 0.3198 - val_acc: 0.9112\n",
            "Epoch 983/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1057 - acc: 0.9641 - val_loss: 0.3323 - val_acc: 0.9075\n",
            "Epoch 984/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1115 - acc: 0.9611 - val_loss: 0.3251 - val_acc: 0.9069\n",
            "Epoch 985/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1049 - acc: 0.9677 - val_loss: 0.3367 - val_acc: 0.9045\n",
            "Epoch 986/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1198 - acc: 0.9575 - val_loss: 0.3080 - val_acc: 0.9155\n",
            "Epoch 987/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1204 - acc: 0.9644 - val_loss: 0.3197 - val_acc: 0.9039\n",
            "Epoch 988/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1146 - acc: 0.9614 - val_loss: 0.3177 - val_acc: 0.9106\n",
            "Epoch 989/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1212 - acc: 0.9593 - val_loss: 0.3273 - val_acc: 0.9130\n",
            "Epoch 990/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1085 - acc: 0.9632 - val_loss: 0.3281 - val_acc: 0.9149\n",
            "Epoch 991/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1120 - acc: 0.9623 - val_loss: 0.3168 - val_acc: 0.9173\n",
            "Epoch 992/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1164 - acc: 0.9614 - val_loss: 0.3072 - val_acc: 0.9149\n",
            "Epoch 993/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1143 - acc: 0.9608 - val_loss: 0.3233 - val_acc: 0.9192\n",
            "Epoch 994/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1148 - acc: 0.9554 - val_loss: 0.3448 - val_acc: 0.9106\n",
            "Epoch 995/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1175 - acc: 0.9626 - val_loss: 0.3383 - val_acc: 0.9039\n",
            "Epoch 996/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1062 - acc: 0.9641 - val_loss: 0.3111 - val_acc: 0.9106\n",
            "Epoch 997/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.1005 - acc: 0.9695 - val_loss: 0.3135 - val_acc: 0.9130\n",
            "Epoch 998/1000\n",
            "3315/3315 [==============================] - 2s 533us/step - loss: 0.0967 - acc: 0.9680 - val_loss: 0.3133 - val_acc: 0.9137\n",
            "Epoch 999/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1057 - acc: 0.9635 - val_loss: 0.3256 - val_acc: 0.9094\n",
            "Epoch 1000/1000\n",
            "3315/3315 [==============================] - 2s 528us/step - loss: 0.1147 - acc: 0.9590 - val_loss: 0.3091 - val_acc: 0.9100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mFytY6LDzgJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "metadata": {
        "id": "TFz4ClZov9gZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "46dcf1d8-4feb-40e0-b4e6-4bc25aeb835d"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4lcaRWYMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f97e66d6fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Vf1W7LgP2DA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "8yyFBt7ASPUe",
        "colab_type": "code",
        "outputId": "3b82ec7a-b61e-4101-a990-0b829f36d254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f97e66796d8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x_ySPOyHxkZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ]
    },
    {
      "metadata": {
        "id": "f5kRmoD-sdHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66353b53-fd46-48b3-b3c8-550806b80f0c"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MNUiznKNwUtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "metadata": {
        "id": "T4oAv6Kx8RBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "cf29c8b7-5aed-41ce-d5b5-d353137f0272"
      },
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHtPzc0Y8hfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checking the accuracy of the loaded model"
      ]
    },
    {
      "metadata": {
        "id": "qUi-Zjuf8hDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9dabf117-af4e-40e6-f106-99aa65c5feae"
      },
      "cell_type": "code",
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1633/1633 [==============================] - 0s 125us/step\n",
            "Restored model, accuracy: 91.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8pXH3y7S9A1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thank you for your attention! To be continued.."
      ]
    }
  ]
}